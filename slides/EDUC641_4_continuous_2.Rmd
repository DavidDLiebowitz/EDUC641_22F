---
title: "Examining the relationship between continuous variables"
subtitle: "EDUC 641: Week XX"
author: "TBD"
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
        # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false

---

```{R, setup, include = F}
library(pacman)
p_load(here, tidyverse, ggplot2, xaringan, knitr, kableExtra, xaringanthemer)

i_am("slides/EDUC641_4_continuous_2.rmd")


red_pink <- "#e64173"
turquoise = "#20B2AA"
orange = "#FFA500"
red = "#fb6107"
blue = "#3b3b9a"
green = "#8bb174"
grey_light = "grey70"
grey_mid = "grey50"
grey_dark = "grey20"
purple = "#6A5ACD"
slate = "#314f4f"

extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".red-pink" = list(color= "red_pink"),
  ".grey-light" = list(color= "grey_light"),
  ".purple" = list(color = "purple"),
  ".small" = list("font-size" = "90%"))

write_extra_css(css = extra_css, outfile = "my_custom.css")


# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 6.75,
  fig.width = 10.5,
  warning = F,
  message = F
)
opts_chunk$set(dev = "svg")
options(device = function(file, width, height) {
  svg(tempfile(), width = width, height = height)
})
options(knitr.table.format = "html")

```
# Roadmap

```{r, out.width = "90%", echo=F}
  include_graphics("Roadmap_4.png")
```

                                                       
---
# Goals of the unit

- Describe relationships between quantitative data that are continuous
- Visualize and substantively describe the relationship between two continuous variables 
- Describe and interpret a fitted bivariate regression line
- Describe and interpret components of a fitted bivariate linear regression model
- Visualize and substantively interpret residuals resulting from a bivariate regression model
- Conduct a statistical inference test of the slope and intercept of a bivariate regression model
- Write R scripts to conduct these analyses
- Articulate modern critiques of null-hypothesis significance testing framework
- Describe strategies to improve replicability and generalizability of quantitative research


---
# Reminder of our motivating question

### .blue[Do individuals living in countries with more total years of attendance in school experience, on average, higher life expectancy?]

#### In other words, we are asking whether the variables *SCHOOLING* and *LIFE_EXPECTANCY* are related.

---
# Materials

### 1. Death penalty data (in file called life_expectancy.csv)
### 2. Codebook describing the contents of said data
### 3. R script to conduct the data analytic tasks of the unit

---
class: middle, inverse
# Bivariate relationships between continuous variables


---
# Recall life expectancy distr.
```{r, echo = F, fig.height=5.5}
who <- read.csv(here::here("data/life_expectancy.csv")) %>%
 janitor::clean_names() %>% 
 filter(year == 2015) %>%
 select(country, schooling, year, life_expectancy) %>% 
 mutate(life_expectancy = round(life_expectancy, digits = 0))
who <- filter(who, !is.na(schooling))
who <- filter(who, !is.na(life_expectancy))

stem(who$life_expectancy)
```

---
# Another way
```{r, echo=F, fig.height=5.5}
ggplot(who, aes(x = life_expectancy)) +
  geom_histogram(binwidth = 1) 
```


---
# What about schooling?
```{r, echo=F, fig.height=5.5}
stem(who$schooling)
```

---
# And differently again
```{r, echo=F, fig.height=5.5}
ggplot(who, aes(x = schooling)) +
  geom_histogram(binwidth = 1) 
```

---
# Numerical univariate statistics

```{r, echo=T}
summary(who$life_expectancy)
summary(who$schooling)
```

<br>

.blue[*Can you interpret the univariate statistics and displays on this and the previous slides?*]

---
# Visualizing the relationship

```{r, echo = F, fig.height=5.5}
ggplot(who, aes(x = schooling, y = life_expectancy)) + 
  geom_label(aes(label=country)) + 
  xlim(0, 22) +
  ylab("Life Expectancy (Yrs)") + xlab("Schooling (Yrs)") +
  scale_y_continuous(breaks = seq(40, 90, 10), limits = c(40, 90))
```

--

Probably easier to see if we have some symbolic way of representing our data...



---
# Visualizing the relationship
```{r, echo=F, fig.height=5.5}
biv <- ggplot(who, aes(x = schooling, y = life_expectancy)) + 
  geom_point() + 
  xlim(0, 22) +
  ylab("Life Expectancy (Yrs)") + xlab("Schooling (Yrs)") +
  scale_y_continuous(breaks = seq(40, 90, 10), limits = c(40, 90))

biv

```

--

Horizontal axis (or *x*-axis) labels the value of the "predictor" *SCHOOLING*. Vertical axis (or *y*-axis) labels the value of the "outcome" *LIFE_EXPECTANCY*

.blue[*Can you interpret the bivariate display? What does it (and does it NOT) say about the relationship between schooling and life expectancy?*]

---
# Visualizing the relationship

```{r, echo = F, fig.height=5.5}
who_chile <- filter(who, country == "Chile")

biv_c <- biv + 
  geom_point(data = who_chile, aes(x=schooling, y = life_expectancy),
                    color = 'red',
                    size = 3) +
  geom_vline(xintercept = 16.3, color = "red", linetype = "dashed", size = 0.75) +
  geom_hline(yintercept = 85, color = "red", linetype = "dashed" , size = 0.75) +
  annotate('text', label = "Chile", x = 15.5, y = 87, color = "red", size = 6) +
  annotate('text', label = "16.3", color = "red", x = 16.9, y = 40, size = 5) +
  annotate('text', label = "85", color = "red", x = 0, y = 87, size =5) 


biv_c

```

--

.blue[*Can you interpret what this display says about the country of Chile?*]

---
# You try...

```{r, echo = F, fig.height=5.5}
who_egypt <- filter(who, country == "Egypt")

biv_c + 
   geom_point(data = who_egypt, aes(x=schooling, y = life_expectancy),
                    color = 'blue',
                    size = 3) +
   annotate('text', label = "Egypt", x = 12.1, y = 79, color = "blue", size = 6)

```

.blue[*Can you interpret what this display says about the country of Egypt?*]

---
# What about the relationship?
```{r, echo = F, fig.height=5.5}
biv
```

.blue[*Is there a relationship between SCHOOLING and LIFE_EXPECTANCY? How do you know?*]

--

.blue[*What kind of line, curve or other construction best summarizes the observed relationship between SCHOOLING and LIFE_EXPECTANCY?*]

---
# An aside about the origin
```{r, echo = F, fig.height=5.5}
bar <- filter(who, country == "Egypt" | country=="Chile")

ggplot(bar, aes(x = country, y = schooling)) + geom_col()
``` 

*Figures that compare measures of central tendency across groups (e.g., bar charts) should always start at zero (0) so as not to artificially inflate the differences between groups*

---
# An aside about the origin

```{r, echo = F, fig.height=5.5}
g1 <- ggplot(who, aes(x = schooling, y = life_expectancy)) + 
  geom_point() + 
  xlim(0, 22) +
  ylab("Life Expectancy (Yrs)") + xlab("Schooling (Yrs)") +
  scale_y_continuous(breaks = seq(0, 90, 10), limits = c(0, 90)) +
  annotate('text', label = "Origin", x = 1, y = 3, color = "red", size =5)

g3 <- ggplot(who, aes(x = schooling, y = life_expectancy)) + 
  geom_point() + 
  xlim(0, 22) +
  ylab("Life Expectancy (Yrs)") + xlab("Schooling (Yrs)") +
  scale_y_continuous(breaks = seq(50, 90, 10), limits = c(50, 90))

gridExtra::grid.arrange(g1, g3, ncol=2)
```

*Figures that describe relationships between two variables (e.g., scatter plots) might (or might not) include the origin (0, 0). The key concept these charts illustrate is the relationship. By adjusting the scale and range of each axis, we can make the relationship "look" different. But the strength and magnitude are the same.* More to come in EDUC 643...

---
class: middle, inverse
# A gentle introduction to bivariate regression: <br> Ordinary-Least Squares (OLS)-fitted regression lines

---
# OLS-fitted regression line
```{r, echo = F, fig.height=5.5}
biv + geom_smooth(method = lm, se = F)
```

.red[*The fitted regression line tells us the best prediction for the values of LIFE_EXPECTANCY.*]

---
# Some intuition
```{r, echo = F, fig.height=5.5}
fit <- lm(life_expectancy ~ schooling, data=who)
who$predict <- predict(fit)
ggplot(who, aes(x = schooling, y = life_expectancy)) + 
  geom_point() + 
  geom_point(aes(y=predict), col = "blue", alpha=0.3) +
  geom_segment(aes(xend = schooling, yend=predict), col = "green", alpha = 0.3) +
  geom_smooth(method = lm, se = F) +
  xlim(0, 22) +
  ylab("Life Expectancy (Yrs)") + xlab("Schooling (Yrs)") +
  scale_y_continuous(breaks = seq(40, 90, 10), limits = c(40, 90))
```

--

Can think of the OLS-fitted regression line as a stick held in place by thumbtacks and elastic bands from each of the data points

---
# A visualization
```{r, echo =F, asis=T}
include_app(here::here("line_SS/app.R"))
## focus only on residuals not SSEs

```

---
# Implementing in R
```{r, echo=T}
fit <- lm(life_expectancy ~ schooling, data=who)
summary(fit)
```
---
# The fitted equation
```{r, echo=F, fig.height=4, highlight.output = (11:12)}
fit <- lm(life_expectancy ~ schooling, data=who)
summary(fit)
```

These "coefficients" tell you where the fitted trend line should be drawn:
$$
\small{
\left[ \textrm{Predicted value of } LIFE\_EXPECTANCY\right]  = 
\left( 42.85 \right) + 2.23 * \left[ \textrm{Observed value of }SCHOOLING  \right] 
}
$$
---
# Fitted values

Can substitute values for the "predictor" $(SCHOOLING)$ into the fitted equation to compute the *predicted* values of $LIFE\_EXPECTANCY$. 
```{r, echo = F, fig.height=5.5}
fit_c <- lm(life_expectancy ~ schooling, data=who)
who$predict_c <- predict(fit_c)
who_chile <- filter(who, country=="Chile")

ggplot(who, aes(x = schooling, y = life_expectancy)) + 
  geom_point() + 
  geom_point(aes(y=predict), data=who_chile, col = "red", alpha=0.3) +
  geom_point(data = who_chile, aes(x=schooling, y = life_expectancy),
                    color = 'red',
                    size = 2) +
  geom_segment(aes(xend = schooling, yend=predict), col = "red", alpha = 0.3, data= who_chile) +
  annotate('text', label = "Chile", x = 15.5, y = 87, color = "red", size = 5) +
  xlim(0, 22) +
  ylab("Life Expectancy (Yrs)") + xlab("Schooling (Yrs)") +
  scale_y_continuous(breaks = seq(40, 90, 10), limits = c(40, 90))
```

--

Can do this for our old friend Chile ... and all others...

---
# Fitted values

So we can re-construct the line of best fit from the fitted values:

```{r, echo = F, fig.height = 5.5}
fit_val <- ggplot(who, aes(x = schooling, y = life_expectancy)) + 
  geom_point() + 
  geom_point(aes(y=predict), col = "red", alpha=0.3) +
  xlim(0, 22) +
  ylab("Life Expectancy (Yrs)") + xlab("Schooling (Yrs)") +
  scale_y_continuous(breaks = seq(40, 90, 10), limits = c(40, 90))
fit_val
```

---
# Fitted values

Note that the fitted line always goes through the average of the predictors
```{r, echo=T}
mean(who$schooling)
mean(who$life_expectancy)
```

---
# Fitted values

Note that the fitted line always goes through the average of the predictors
```{r, echo = F, fig.height = 5.5}
fit_val +
  geom_vline(xintercept = 12.93, col = "red", linetype="dashed") +
  geom_hline(yintercept = 71.74, col = "red", linetype= "dashed") +
  annotate('text', label = "(12.93,", x = 13.7, y = 68.5, color = "red", size = 4) +
  annotate('text', label = "71.74)", x = 13.7, y = 65.5, color = "red", size =4)
```
---
# The regression equation

Each term in the regression equation has a specific interpretation

$$
\hat{LIFE\_EXPECTANCY} = 42.85 + 2.23 * \left( SCHOOLING \right)
$$

---
# The regression equation

Each term in the regression equation has a specific interpretation:

$$
\color{red}{\hat{LIFE\_EXPECTANCY}} = 42.85 + 2.23 * \left( SCHOOLING \right)
$$

The predicted value of $\color{red}{\hat{LIFE\_EXPECTANCY}}$ is based on the OLS regression fit. Its "hat" represents that it is a prediction.

---
# The regression equation

Each term in the regression equation has a specific interpretation:

$$
\hat{LIFE\_EXPECTANCY} = \color{red}{42.85} + 2.23 * \left( SCHOOLING \right)
$$

42.85 represents the .red[*estimated intercept*]. It tells you the predicted value of $LIFE\_EXPECTANCY$ when $SCHOOLING$ is zero (0)

- .blue[*In this context, it doesn't make sense to interpret this. Why?*]

---
# The regression equation

Each term in the regression equation has a specific interpretation:

$$
\hat{LIFE\_EXPECTANCY} = 42.85 + \color{red}{2.23} * \left( SCHOOLING \right)
$$

--

2.23 represents the .red[*estimated slope*]. It summarizes the relationship between $LIFE\_EXPECTANCY$ and $SCHOOLING$. It tells you the difference in the predicted values of $LIFE\_EXPECTANCY$ *per unit difference* in  $SCHOOLING$.

--

Slopes can be positive (as in this case) or negative. Here, we conclude that countries that experience one additional year of schooling have an average life expectancy of 2.23 more years.


---
# The regression equation

Each term in the regression equation has a specific interpretation:

$$
\hat{LIFE\_EXPECTANCY} = 42.85 + 2.23 * \left( \color{red}{SCHOOLING} \right)
$$

--

$\color{red}{SCHOOLING}$ represents the .red[*actual values*] of the predictor $SCHOOLING$.

---
# Regression inference

As with our categorical and single-variable continuous data analysis, we can ask whether we might have observed a relationship between $LIFE\_EXPECTANCY$ and $SCHOOLING$ by an idiosyncratic accident of sampling.

--

Could we have gotten a slope value of 2.23 by sampling from a population in which there was **no relationship** between $LIFE\_EXPECTANCY$ and $SCHOOLING$?

- In other words, by sampling from a *null population* in which the slope was zero?

---
# Regression inference

Could we have gotten a slope value of 2.23 by sampling from a population in which there was **no relationship** between $LIFE\_EXPECTANCY$ and $SCHOOLING$?

```{r, echo=F, highlight.output = (11:14)}
summary(fit)
```

--

As with our previous analysis, R provides us with a *p*-value which can help us to judge the likelihood that our results are driven by idiosyncrasies of sampling

---
# Regression inference

```{r, echo=F, highlight.output = (11:14)}
summary(fit)
```

Here, the *p*-value for the $\frac{LIFE\_EXPECTANCY}{SCHOOLING}$ regression slope is $<0.0001$ (in fact, $<2^{-16}$). 

With an alpha-threshold of 0.05, $2^{-16}$ is definitely less than 0.05. Thus, we reject the null hypothesis that there is no relationship  between $LIFE\_EXPECTANCY$ and $SCHOOLING$, on average in the population.

---
# Writing it up
.pull-left[
```{r, echo=F}
ggplot(who, aes(x = schooling, y = life_expectancy)) + 
  geom_point() + 
  geom_smooth(method = lm, se = F) +
  xlim(0, 22) +
  ylab("Life Expectancy (Yrs)") + xlab("Schooling (Yrs)") +
  scale_y_continuous(breaks = seq(40, 90, 10), limits = c(40, 90))
```
]

.pull-right[
> **The story so far** <br> <font size="2"> In our investigation of country-level aggregate measures of schooling and life expectancy, we have found that the average years of schooling in a country is related to the average life expectancy. In particular, when we relate the country-level life expectancy (*LIFE_EXPECTANCY*) to the country-level mean years of schooling (*SCHOOLING*), we find that the trend-line estimated by ordinary-least-squares regression has a slope of 2.23 (*p*<0.0001). This suggests that two countries that differ in their average years of schooling attainment by 1 year will have, on average, a difference in life expectancy of 2.23 years. Of course, this relationship is far from causal... </font>

]
---
class: middle, inverse

# A gentle introduction to bivariate regression: <br> Residual analysis

---
# Residual analysis
```{r, echo=F, fig.height=5}
ggplot(who, aes(x = schooling, y = life_expectancy)) + 
  geom_point() + 
  geom_point(aes(y=predict), col = "blue", alpha=0.3) +
  geom_segment(aes(xend = schooling, yend=predict), col = "green", alpha = 0.3) +
  geom_smooth(method = lm, se = F) +
  xlim(0, 22) +
  ylab("Life Expectancy (Yrs)") + xlab("Schooling (Yrs)") +
  scale_y_continuous(breaks = seq(40, 90, 10), limits = c(40, 90))
```

Our fitted regression line contains the "predicted" values of *LIFE_EXPECTANCY* for each value of *SCHOOLING*. But almost all of the "actual" values of *LIFE_EXPECTANCY* lie off the actual line regression line.

---
# An example: Chile

```{r, echo=F, fig.height=5}
resid_c <- ggplot(who, aes(x = schooling, y = life_expectancy)) + 
  geom_point() + 
  geom_smooth(method=lm, se=F, data=who) +
  geom_point(aes(y=predict), data=who_chile, col = "red", alpha=0.3) +
  geom_point(data = who_chile, aes(x=schooling, y = life_expectancy),
                    color = 'red',
                    size = 2) +
  geom_segment(aes(xend = schooling, yend=predict), col = "red", alpha = 0.3, data= who_chile) +
  annotate('text', label = "Chile", x = 15.5, y = 87, color = "red", size = 5) +
  xlim(0, 22) +
  ylab("Life Expectancy (Yrs)") + xlab("Schooling (Yrs)") +
  scale_y_continuous(breaks = seq(40, 90, 10), limits = c(40, 90))
resid_c
```

Observed values for Chile: $LIFE\_EXPECTANCY = 85$; $SCHOOLING = 16.3$ <br>
Predicted value of *LIFE_EXPECTANCY* for Chile:
$$
\begin{align}
\hat{LIFE\_EXPECTANCY} & = 42.85 + 2.23 * (16.3) \\
& = 79.20
\end{align}
$$
---
# An example: Chile
```{r, echo=F, fig.height=5}
resid_c
```
$\hat{LIFE\_EXPECTANCY} = 79.20$ <br>
Actual life expectancy = 85

--

.blue[*What can we say about the country of Chile's average life expectancy, relative to our prediction?*] 

---
# Now Egypt
```{r, echo=F, fig.height=5}
resid_c +
   geom_point(data = who_egypt, aes(x=schooling, y = life_expectancy),
                    color = 'blue',
                    size = 3) +
   annotate('text', label = "Egypt", x = 12.1, y = 79, color = "blue", size = 5)
```
Observed values for Egypt: $LIFE\_EXPECTANCY = 79$; $SCHOOLING = 13.1$ <br>

.blue[Can you calculate the predicted value of *LIFE_EXPECTANCY* for Egypt and compare it to the observed?]

---
# What is a "residual"?

#### The difference ("vertical distance") between the observed value of the outcome its predicted value is called the *residual*.

#### Residuals can be substantively and statistically useful:
- Represent individual deviations from average trend
- Tell us about values of the outcome after taking into account ("adjusting for") the predictor
 + In this case, tell us whether countries have better or worse life expectancies, given their average years of schooling
 
---
# Residual analysis

```{r, echo=T}
fit <- lm(life_expectancy ~ schooling, data=who)

# predict asks for the predicted values
who$predict <- predict(fit)

# resid asks for the raw residual
who$resid <- residuals(fit)
```

We can now treat these residual and predicted values as new variables in our dataset and examine using all the other univariate and multivariate analysis tools we have.

---
# Examining the residuals

```{r, echo=T}
summary(who$resid)
```

- Sample mean of the residuals is *always* exactly zero
- We've done a very poor job of predicting life expectancy for some countries

--

```{r, echo=T}
sd(who$resid)
```

- Standard deviation of the raw residuals can be quite useful in checking our assumptions
 + .blue[*What assumption?*]
 
---
# Residual assumptions

For the *p*-values that we computed in the regression analysis to be correct, the residuals **must be normally distributed**

```{r, echo=T, fig.height=4.5}
boxplot(resid(fit))
```

--

A few outliers, but we seem to be doing ok...

---
# Residual assumptions

For the *p*-values that we computed in the regression analysis to be correct, the residuals **must be normally distributed**

```{r, echo=T, fig.height=4.5}
ggplot(who, aes(x = resid)) + 
  geom_histogram(binwidth = 1)
```

--

Pretty good, pretty good...


---
# Residual vs. fitted plot 

For the *p*-values that we computed in the regression analysis to be correct, the residuals **must be normally distributed**
```{r, echo=F, fig.height=4.5}
ggplot(who, aes(x = predict, y = resid)) + 
         geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype="dashed") +
  ylab("Residuals") + xlab("Fitted values") +
  scale_y_continuous(limits=c(-20, 20))
```

#### Key assumption checks for normality:
- The residuals "bounce randomly" around the 0 line. 
- The residuals could be roughly contained within a rectangle around the 0 line. 
- No one residual "stands out" from the basic random pattern of residuals.

---
# Residual vs. fitted plot

For the *p*-values that we computed in the regression analysis to be correct, the residuals **must be normally distributed**
```{r, echo=F, fig.height=4.5}
ggplot(who, aes(x = predict, y = resid)) + 
         geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype="dashed") +
  annotate('rect', xmin = 53.8, xmax = 59.5, ymin = -10, ymax = 15, alpha = 0.3, fill = "purple") +
  ylab("Residuals") + xlab("Fitted values") +
  scale_y_continuous(limits=c(-20, 20))
```

#### Key assumption checks for normality:
- The residuals "bounce randomly" around the 0 line. 
- The residuals could be roughly contained within a rectangle around the 0 line. 
- No one residual "stands out" from the basic random pattern of residuals.

---
# Writing it up

> <font size="2"> In our investigation of country-level aggregate measures of schooling and life expectancy, we have found that the average years of schooling in a country is related to the average life expectancy. In particular, when we relate the country-level life expectancy (*LIFE_EXPECTANCY*) to the country-level mean years of schooling (*SCHOOLING*), we find that the trend-line estimated by ordinary-least-squares regression has a slope of 2.23 (*p*<0.0001). This suggests that two countries that differ in their average years of schooling attainment by 1 year will have, on average, a difference in life expectancy of 2.23 years. Of course, this relationship is far from causal... </font> <br><br>

--

> <font size="2"> An analysis of the residuals from our fitted model suggests that our regression assumptions are reasonably well met and we have appropriately characterized the relationship between schooling and life expectancy. Despite the presence of a few outliers, our residuals are roughly symmetrically distributed around 0. Our fitted regression does seem to underpredict life expectancy for very low levels of schooling. </font>

---
# Key takeaways

- **Understand your data first**
 + Summarize and visualize each variable independently
 + Start with a visual representation of the relationship between your two variables
 + How you display the relationship will influence your perception of the relationship, but will not change the relationship
 + Try to describe what a particular observation in your visualized data represents
- **The regression model represents your hypothesis about the population**
 + When you fit a regression model, you are estimating *sample* values of *population* parameters that you will not directly observe
 + The goal of classical regression inference (just as with categorical relationships) is to understand how likely the observed data in your sample are in the presence of no relationship in the unobserved population
- **The regression model has a "smooth" and a "rough" component to it**
 + The "smooth" part is the portion of the relationship that your model explains
 + The "rough" part is the extent to which each observation (and the observations in aggregate) vary from the "smooth" part of your predictions
 + The "rough" parts (the residuals) provide important information on the extent to which our models satisfy their assumptions
 
.blue[**More on all of this in EDUC 643**]

---
class: middle, inverse
# Synthesis and wrap-up



