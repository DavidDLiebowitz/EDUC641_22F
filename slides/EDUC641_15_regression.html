<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Examining the Relationship Between Continuous Variables</title>
    <meta charset="utf-8" />
    <meta name="author" content="David D. Liebowitz" />
    <script src="EDUC641_15_regression_files/header-attrs-2.14.3/header-attrs.js"></script>
    <link href="EDUC641_15_regression_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="EDUC641_15_regression_files/remark-css-0.0.1/uo.css" rel="stylesheet" />
    <link href="EDUC641_15_regression_files/remark-css-0.0.1/ki-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my_custom.css" type="text/css" />
    <link rel="stylesheet" href="xaringanthemer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Examining the Relationship Between Continuous Variables
]
.subtitle[
## EDUC 641: Unit 4 Part 2
]
.author[
### David D. Liebowitz
]

---



# Roadmap

&lt;img src="Roadmap_4.png" width="90%" style="display: block; margin: auto;" /&gt;

                                                       
---
# Goals of the unit

- Describe relationships between quantitative data that are continuous
- Visualize and substantively describe the relationship between two continuous variables 
- Describe and interpret a fitted bivariate regression line
- Describe and interpret components of a fitted bivariate linear regression model
- Visualize and substantively interpret residuals resulting from a bivariate regression model
- Conduct a statistical inference test of the slope and intercept of a bivariate regression model
- Write R scripts to conduct these analyses


---
# Reminder of motivating question

#### We learned a lot about the distribution of life expectancy in countries, now we are turning to thinking about relationships between life expectancy and other variables. In particular:

#### .blue[Do individuals living in countries with more total years of attendance in school experience, on average, higher life expectancy?]

#### In other words, we are asking whether the variables *SCHOOLING* and *LIFE_EXPECTANCY* are related.

---
# Materials

### 1. Death penalty data (in file called life_expectancy.csv)
### 2. Codebook describing the contents of said data
### 3. R script to conduct the data analytic tasks of the unit

---
class: middle, inverse
# A gentle introduction to bivariate regression: &lt;br&gt; Ordinary-Least Squares (OLS)-fitted regression lines

---
# A relationship reminder
&lt;img src="EDUC641_15_regression_files/figure-html/unnamed-chunk-2-1.svg" style="display: block; margin: auto;" /&gt;
---
# OLS-fitted regression line
&lt;img src="EDUC641_15_regression_files/figure-html/unnamed-chunk-3-1.svg" style="display: block; margin: auto;" /&gt;

.red[*The fitted regression line tells us the best prediction for the values of LIFE_EXPECTANCY.*]

---
# Some intuition
&lt;img src="EDUC641_15_regression_files/figure-html/unnamed-chunk-4-1.svg" style="display: block; margin: auto;" /&gt;

--

Can think of the OLS-fitted regression line as a stick held in place by thumbtacks and elastic bands from each of the data points

---
# A visualization
&lt;iframe src="https://daviddl.shinyapps.io/line_ss/?showcase=0" width="100%" height="550px" data-external="1"&gt;&lt;/iframe&gt;

---
# Implementing in R

```r
fit &lt;- lm(life_expectancy ~ schooling, data=who)
summary(fit)
```

```
#&gt; 
#&gt; Call:
#&gt; lm(formula = life_expectancy ~ schooling, data = who)
#&gt; 
#&gt; Residuals:
#&gt;      Min       1Q   Median       3Q      Max 
#&gt; -16.3270  -2.6565   0.1581   3.3095  10.9758 
#&gt; 
#&gt; Coefficients:
#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)  42.8501     1.5976   26.82   &lt;2e-16 ***
#&gt; schooling     2.2348     0.1206   18.53   &lt;2e-16 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 4.606 on 171 degrees of freedom
#&gt; Multiple R-squared:  0.6676,	Adjusted R-squared:  0.6657 
#&gt; F-statistic: 343.5 on 1 and 171 DF,  p-value: &lt; 2.2e-16
```
---
# The fitted equation

```
#&gt; 
#&gt; Call:
#&gt; lm(formula = life_expectancy ~ schooling, data = who)
#&gt; 
#&gt; Residuals:
#&gt;      Min       1Q   Median       3Q      Max 
#&gt; -16.3270  -2.6565   0.1581   3.3095  10.9758 
#&gt; 
#&gt; Coefficients:
#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    
*#&gt; (Intercept)  42.8501     1.5976   26.82   &lt;2e-16 ***
*#&gt; schooling     2.2348     0.1206   18.53   &lt;2e-16 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 4.606 on 171 degrees of freedom
#&gt; Multiple R-squared:  0.6676,	Adjusted R-squared:  0.6657 
#&gt; F-statistic: 343.5 on 1 and 171 DF,  p-value: &lt; 2.2e-16
```

These "coefficients" tell you where the fitted trend line should be drawn:
$$
\small{
\left[ \textrm{Predicted value of } LIFE\_EXPECTANCY\right]  = 
\left( 42.85 \right) + 2.23 * \left[ \textrm{Observed value of }SCHOOLING  \right] 
}
$$
---
# Fitted values

Can substitute values for the "predictor" `\((SCHOOLING)\)` into the fitted equation to compute the *predicted* values of `\(LIFE\_EXPECTANCY\)`. 
&lt;img src="EDUC641_15_regression_files/figure-html/unnamed-chunk-8-1.svg" style="display: block; margin: auto;" /&gt;

--

Can do this for our old friend Chile ... and all others...

---
# Fitted values

So we can re-construct the line of best fit from the fitted values:

&lt;img src="EDUC641_15_regression_files/figure-html/unnamed-chunk-9-1.svg" style="display: block; margin: auto;" /&gt;

---
# Fitted values

Note that the fitted line always goes through the average of the predictors

```r
mean(who$schooling)
```

```
#&gt; [1] 12.92717
```

```r
mean(who$life_expectancy)
```

```
#&gt; [1] 71.73988
```

---
# Fitted values

Note that the fitted line always goes through the average of the predictors
&lt;img src="EDUC641_15_regression_files/figure-html/unnamed-chunk-11-1.svg" style="display: block; margin: auto;" /&gt;
---
# The regression equation

Each term in the regression equation has a specific interpretation

$$
\hat{LIFE\_EXPECTANCY} = 42.85 + 2.23 * \left( SCHOOLING \right)
$$

---
# The regression equation

Each term in the regression equation has a specific interpretation:

$$
\color{red}{\hat{LIFE\_EXPECTANCY}} = 42.85 + 2.23 * \left( SCHOOLING \right)
$$

The predicted value of `\(\color{red}{\hat{LIFE\_EXPECTANCY}}\)` is based on the OLS regression fit. Its "hat" represents that it is a prediction.

---
# The regression equation

Each term in the regression equation has a specific interpretation:

$$
\hat{LIFE\_EXPECTANCY} = \color{red}{42.85} + 2.23 * \left( SCHOOLING \right)
$$

42.85 represents the .red[*estimated intercept*]. It tells you the predicted value of `\(LIFE\_EXPECTANCY\)` when `\(SCHOOLING\)` is zero (0)

- .blue[*In this context, it doesn't make sense to interpret this. Why?*]

---
# The regression equation

Each term in the regression equation has a specific interpretation:

$$
\hat{LIFE\_EXPECTANCY} = 42.85 + \color{red}{2.23} * \left( SCHOOLING \right)
$$

--

2.23 represents the .red[*estimated slope*]. It summarizes the relationship between `\(LIFE\_EXPECTANCY\)` and `\(SCHOOLING\)`. It tells you the difference in the predicted values of `\(LIFE\_EXPECTANCY\)` *per unit difference* in  `\(SCHOOLING\)`.

--

Slopes can be positive (as in this case) or negative. Here, we conclude that countries that experience one additional year of schooling have an average life expectancy of 2.23 more years.


---
# The regression equation

Each term in the regression equation has a specific interpretation:

$$
\hat{LIFE\_EXPECTANCY} = 42.85 + 2.23 * \left( \color{red}{SCHOOLING} \right)
$$

--

`\(\color{red}{SCHOOLING}\)` represents the .red[*actual values*] of the predictor `\(SCHOOLING\)`.

---
# Regression inference

As with our categorical and single-variable continuous data analysis, we can ask whether we might have observed a relationship between `\(LIFE\_EXPECTANCY\)` and `\(SCHOOLING\)` by an idiosyncratic accident of sampling.

--

Could we have gotten a slope value of 2.23 by sampling from a population in which there was **no relationship** between `\(LIFE\_EXPECTANCY\)` and `\(SCHOOLING\)`?

- In other words, by sampling from a *null population* in which the slope was zero?

---
# Regression inference

.small[Could we have gotten a slope value of 2.23 by sampling from a population in which there was **no relationship** between `\(LIFE\_EXPECTANCY\)` and `\(SCHOOLING\)`?]


```
#&gt; 
#&gt; Call:
#&gt; lm(formula = life_expectancy ~ schooling, data = who)
#&gt; 
#&gt; Residuals:
#&gt;      Min       1Q   Median       3Q      Max 
#&gt; -16.3270  -2.6565   0.1581   3.3095  10.9758 
#&gt; 
#&gt; Coefficients:
#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    
*#&gt; (Intercept)  42.8501     1.5976   26.82   &lt;2e-16 ***
*#&gt; schooling     2.2348     0.1206   18.53   &lt;2e-16 ***
*#&gt; ---
*#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 4.606 on 171 degrees of freedom
#&gt; Multiple R-squared:  0.6676,	Adjusted R-squared:  0.6657 
#&gt; F-statistic: 343.5 on 1 and 171 DF,  p-value: &lt; 2.2e-16
```

--

.small[As with our previous analysis, R provides us with a *p*-value which can help us to judge the likelihood that our results are driven by idiosyncrasies of sampling]

---
# Regression inference


```
#&gt; 
#&gt; Call:
#&gt; lm(formula = life_expectancy ~ schooling, data = who)
#&gt; 
#&gt; Residuals:
#&gt;      Min       1Q   Median       3Q      Max 
#&gt; -16.3270  -2.6565   0.1581   3.3095  10.9758 
#&gt; 
#&gt; Coefficients:
#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    
*#&gt; (Intercept)  42.8501     1.5976   26.82   &lt;2e-16 ***
*#&gt; schooling     2.2348     0.1206   18.53   &lt;2e-16 ***
*#&gt; ---
*#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 4.606 on 171 degrees of freedom
#&gt; Multiple R-squared:  0.6676,	Adjusted R-squared:  0.6657 
#&gt; F-statistic: 343.5 on 1 and 171 DF,  p-value: &lt; 2.2e-16
```

Here, the *p*-value for the `\(\frac{LIFE\_EXPECTANCY}{SCHOOLING}\)` regression slope is `\(&lt;0.0001\)` (in fact, `\(&lt;2^{-16}\)`). 

With an alpha-threshold of 0.05, `\(2^{-16}\)` is definitely less than 0.05. Thus, we reject the null hypothesis that there is no relationship  between `\(LIFE\_EXPECTANCY\)` and `\(SCHOOLING\)`, on average in the population.

---
# Writing it up
.pull-left[
&lt;img src="EDUC641_15_regression_files/figure-html/unnamed-chunk-14-1.svg" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&gt; **The story so far** &lt;br&gt; .small[In our investigation of country-level aggregate measures of schooling and life expectancy, we have found that the average years of schooling in a country is related to the average life expectancy. In particular, when we relate the country-level life expectancy (*LIFE_EXPECTANCY*) to the country-level mean years of schooling (*SCHOOLING*), we find that the trend-line estimated by ordinary-least-squares regression has a slope of 2.23 (*p*&lt;0.0001). This suggests that two countries that differ in their average years of schooling attainment by 1 year will have, on average, a difference in life expectancy of 2.23 years. Of course, this relationship is far from causal...]

]
---
class: middle, inverse

# A gentle introduction to bivariate regression: &lt;br&gt; Residual analysis

---
# Residual analysis
&lt;img src="EDUC641_15_regression_files/figure-html/unnamed-chunk-15-1.svg" style="display: block; margin: auto;" /&gt;

Our fitted regression line contains the "predicted" values of *LIFE_EXPECTANCY* for each value of *SCHOOLING*. But almost all of the "actual" values of *LIFE_EXPECTANCY* lie off the actual line regression line.

---
# An example: Chile

&lt;img src="EDUC641_15_regression_files/figure-html/unnamed-chunk-16-1.svg" style="display: block; margin: auto;" /&gt;

Observed values for Chile: `\(LIFE\_EXPECTANCY = 85\)`; `\(SCHOOLING = 16.3\)` &lt;br&gt;
Predicted value of *LIFE_EXPECTANCY* for Chile:
$$
`\begin{align}
\hat{LIFE\_EXPECTANCY} &amp; = 42.85 + 2.23 * (16.3) \\
&amp; = 79.20
\end{align}`
$$
---
# An example: Chile
&lt;img src="EDUC641_15_regression_files/figure-html/unnamed-chunk-17-1.svg" style="display: block; margin: auto;" /&gt;
`\(\hat{LIFE\_EXPECTANCY} = 79.20\)` &lt;br&gt;
Actual life expectancy = 85

--

.blue[*What can we say about the country of Chile's average life expectancy, relative to our prediction?*] 

---
# Now Egypt
&lt;img src="EDUC641_15_regression_files/figure-html/unnamed-chunk-18-1.svg" style="display: block; margin: auto;" /&gt;
Observed values for Egypt: `\(LIFE\_EXPECTANCY = 79\)`; `\(SCHOOLING = 13.1\)` &lt;br&gt;

.blue[Can you calculate the predicted value of *LIFE_EXPECTANCY* for Egypt and compare it to the observed?]

---
# What is a "residual"?

#### The difference ("vertical distance") between the observed value of the outcome its predicted value is called the *residual*.

#### Residuals can be substantively and statistically useful:
- Represent individual deviations from average trend
- Tell us about values of the outcome after taking into account ("adjusting for") the predictor
 + In this case, tell us whether countries have better or worse life expectancies, given their average years of schooling
 
---
# Residual analysis


```r
fit &lt;- lm(life_expectancy ~ schooling, data=who)

# predict asks for the predicted values
who$predict &lt;- predict(fit)

# resid asks for the raw residual
who$resid &lt;- residuals(fit)
```

We can now treat these residual and predicted values as new variables in our dataset and examine using all the other univariate and multivariate analysis tools we have.

---
# Examining the residuals


```r
summary(who$resid)
```

```
#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
#&gt; -16.3270  -2.6565   0.1581   0.0000   3.3095  10.9758
```

- Sample mean of the residuals is *always* exactly zero
- We've done a very poor job of predicting life expectancy for some countries

--


```r
sd(who$resid)
```

```
#&gt; [1] 4.592143
```

- Standard deviation of the raw residuals can be quite useful in checking our assumptions
 + .blue[*What assumption?*]
 
---
# Residual assumptions

For the *p*-values that we computed in the regression analysis to be correct, the residuals **must be normally distributed**


```r
boxplot(resid(fit))
```

&lt;img src="EDUC641_15_regression_files/figure-html/unnamed-chunk-22-1.svg" style="display: block; margin: auto;" /&gt;

--

A few outliers, but we seem to be doing ok...

---
# Residual assumptions

For the *p*-values that we computed in the regression analysis to be correct, the residuals **must be normally distributed**


```r
ggplot(who, aes(x = resid)) + 
  geom_histogram(binwidth = 1) +
    theme_minimal(base_size = 16)
```

&lt;img src="EDUC641_15_regression_files/figure-html/unnamed-chunk-23-1.svg" style="display: block; margin: auto;" /&gt;

--

Pretty good, pretty good...


---
# Residual vs. fitted plot 

For the *p*-values that we computed in the regression analysis to be correct, the residuals **must be normally distributed**
&lt;img src="EDUC641_15_regression_files/figure-html/unnamed-chunk-24-1.svg" style="display: block; margin: auto;" /&gt;

#### Key assumption checks for normality:
- The residuals "bounce randomly" around the 0 line. 
- The residuals could be roughly contained within a rectangle around the 0 line. 
- No one residual "stands out" from the basic random pattern of residuals.

---
# Residual vs. fitted plot

For the *p*-values that we computed in the regression analysis to be correct, the residuals **must be normally distributed**
&lt;img src="EDUC641_15_regression_files/figure-html/unnamed-chunk-25-1.svg" style="display: block; margin: auto;" /&gt;

#### Key assumption checks for normality:
- The residuals "bounce randomly" around the 0 line. 
- The residuals could be roughly contained within a rectangle around the 0 line. 
- No one residual "stands out" from the basic random pattern of residuals.

---
# Writing it up

&gt; .small[In our investigation of country-level aggregate measures of schooling and life expectancy, we have found that the average years of schooling in a country is related to the average life expectancy. In particular, when we relate the country-level life expectancy (*LIFE_EXPECTANCY*) to the country-level mean years of schooling (*SCHOOLING*), we find that the trend-line estimated by ordinary-least-squares regression has a slope of 2.23 (*p*&lt;0.0001). This suggests that two countries that differ in their average years of schooling attainment by 1 year will have, on average, a difference in life expectancy of 2.23 years. Of course, this relationship is far from causal...] 

&lt;br&gt;&lt;br&gt;

--

&gt; .small[An analysis of the residuals from our fitted model suggests that our regression assumptions are reasonably well met and we have appropriately characterized the relationship between schooling and life expectancy. Despite the presence of a few outliers, our residuals are roughly symmetrically distributed around 0. Our fitted regression does seem to underpredict life expectancy for very low levels of schooling.]

---
# Key takeaways

- **Understand your data first**
 + Summarize and visualize each variable independently
 + Start with a visual representation of the relationship between your two variables
 + How you display the relationship will influence your perception of the relationship, but will not change the relationship
 + Try to describe what a particular observation in your visualized data represents
- **The regression model represents your hypothesis about the population**
 + When you fit a regression model, you are estimating *sample* values of *population* parameters that you will not directly observe
 + The goal of classical regression inference (just as with categorical relationships) is to understand how likely the observed data in your sample are in the presence of no relationship in the unobserved population
- **The regression model has a "smooth" and a "rough" component to it**
 + The "smooth" part is the portion of the relationship that your model explains
 + The "rough" part is the extent to which each observation (and the observations in aggregate) vary from the "smooth" part of your predictions
 + The "rough" parts (the residuals) provide important information on the extent to which our models satisfy their assumptions
 
.blue[**More on all of this in EDUC 643**]

---
class: middle, inverse
# Synthesis and wrap-up


---
# Goals of the unit

- Describe relationships between quantitative data that are continuous
- Visualize and substantively describe the relationship between two continuous variables 
- Describe and interpret a fitted bivariate regression line
- Describe and interpret components of a fitted bivariate linear regression model
- Visualize and substantively interpret residuals resulting from a bivariate regression model
- Conduct a statistical inference test of the slope and intercept of a bivariate regression model
- Write R scripts to conduct these analyses

---
# To Dos

### Reading
LSWR Chapter 15.1 and 15.2: bivariate regression

### Assignments
Assignment #4 Due November 28 at 11:59PM
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
