---
title: "EDUC 641 Continuous Data"
author: "Professor Name"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    css: ['default','new.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
  html_document:
    keep_md: true
---
class: inverse, center, middle

```{r setup, include = FALSE}
options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(warning = FALSE,
                      message = FALSE,
                      fig.align = "center",
                      fig.height = 3)

if (!require(pacman)) install.packages('pacman', repos = 'https://cran.rstudio.com')
pacman::p_load(tidyverse, here, knitr, dplyr, ggplot2, blogdown, rio, gifski)
```

```{r, echo = F}
who <- import(here::here("data", "life_expectancy.csv")) %>% 
  janitor::clean_names() %>% 
  filter(year == 2015) %>%
  select(country, status, year, life_expectancy) %>% 
  rename(region = country) %>% 
  mutate(life_expectancy = round(life_expectancy, digits = 0))

```

# Agenda
* Sampling Distributions
* Central Limit Theorem

---
# Normal Distributions

Characterized by:

* Central Tendency
  + Mean
  + Median
  + Mode

* Variability
  + Range
  + Interquartile Range
  + Variance
  + Standard Deviation

* Shape
  + Skew
  + Kurtosis

---
# Normal Distributions

A normal distribution is a **theoretical** distribution with the following characteristics:

Central Tendency:
+ The mean, median, and mode are equivalent values.

Variability:
+ 68% of observations are within 1 SD
+ 95% of  observations are within 2 SDs
+ 99% of observations are within 3 SDs

Shape: 
+ Symmetrical

---
# Z-unit Distribution

* A normal distribution can have any mean and any (positive) standard deviation.
* A z normal distribution is a useful distribution with a mean of 0 $(\mu)$ and a standard deviation $(\sigma)$ of 1.

```{r echo=FALSE}
sim_data <- data_frame(value = rnorm(n = 1000000, mean = 0, sd = 1))

ggplot(sim_data, aes(value)) +
  geom_density(adjust = 3) +
  scale_x_continuous(limits = c(-4, 4),
                     breaks = c(seq(-4, 4, 1))) +
  ylim(0, 0.4) +
  theme_classic() +
  ggtitle("Z Distribution", subtitle =  "m = 0, sd = 1") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))
```


---
# Transforming Distributions
* Any normal distribution can be transformed into a z distribution using a linear function.

Each observation is transformed into a **z-score** using the following formula:
$$z = \frac{X - \mu}{\sigma}$$

* A z-score is calculated by **subtracting the mean** from each value and **dividing by the standard deviation**.

* An observations z-score value is equal to its deviation from the mean, as measured in standard deviation units.

---
# Transforming Distributions

Here is a histogram of our life expectancy data.
```{r, echo = F}
hist(who$life_expectancy)
```

We are going to create a new variable called `life_expectancy_zscore` using the linear function described on the previous slide.
```{r}
who$life_expectancy_zscore <- (who$life_expectancy - mean(who$life_expectancy))/sd(who$life_expectancy)
```

---
# Transforming Distributions

Let's look at the first few transformed values.
```{r}
head(who$life_expectancy_zscore)
```

```{r}
## Histogram of the new z-scores
hist(who$life_expectancy_zscore)
```

Our new mean is approximately 0.
```{r}
mean(who$life_expectancy_zscore)
```

Our new standard deviation is 1.
```{r}
sd(who$life_expectancy_zscore)
```

---
# Normal Distribution

* The area under a normal distribution is always equal to 1.
* The area under the curve corresponds to the proportion of observations.
* The percentage of observations in any part of a normal distribution is always known. 

```{r, echo = F}
sim_data <- data_frame(value = rnorm(n = 1000000, mean = 0, sd = 1))

ggplot(sim_data, aes(value)) +
  geom_density(adjust = 3) +
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = .38)) +
  geom_segment(aes(x = -1, xend = -1, y = 0, yend = .24)) +
  geom_segment(aes(x = 1, xend = 1, y = 0, yend = .24)) +
  geom_segment(aes(x = -2, xend = -2, y = 0, yend = .056)) +
  geom_segment(aes(x = 2, xend = 2, y = 0, yend = .056)) +
  geom_segment(aes(x = -3, xend = -3, y = 0, yend = .005)) +
  geom_segment(aes(x = 3, xend = 3, y = 0, yend = .005)) +
  annotate("text", x = -.5, y = .2, label = "34%") +
  annotate("text", x = .5, y = .2, label = "34%") +
  annotate("text", x = -1.5, y = .05, label = "14%") +
  annotate("text", x = 1.5, y = .05, label = "14%") +
  annotate("text", x = -2.5, y = .05, label = "2%") +
  annotate("text", x = 2.5, y = .05, label = "2%") +
  annotate("text", x = -3.5, y = .05, label = "<1%") +
  annotate("text", x = 3.5, y = .05, label = "<1%") +
  scale_x_continuous(limits = c(-4, 4),
                     breaks = c(seq(-4, 4, 1))) +
  ylim(0, 0.4) +
  theme_classic()

```

What percentage of observations fall at least 1 standard deviation below the mean?

---
# Population vs. Sample Distributions
 
Using the WHO dataset, let's say we want to identify the mean life expectancy across all countries. However, let's also assume we only have the resources to sample from 10 countries.

.pull-left[

Our **population statistic** represents the true value across all countries.

```{r, echo = F}
world <- map_data("world")

regions <- world %>% 
  distinct(region) %>% 
  sample_n(10) %>% 
  mutate(sample = 1)

world <- left_join(world, regions) %>% 
  mutate(population = 1)
```

```{r, echo = F, fig.height=3, fig.width= 4}
ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region, fill = as.factor(population)),
    color = "black", size = 0.1
  ) +
  theme(legend.position = "none")
```
]

.pull-right[
Our **sample statistic** will be our estimate of the population statistic using the countries we examine.

```{r, echo = F, fig.height = 3, fig.width = 4}
ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region, fill = as.factor(sample)),
    color = "black", size = 0.1
  ) +
  theme(legend.position = "none")
```
]

---
# Sample Estimates

* Assume that our <span style="color: blue;">true population statistic </span> is 71.64. 

* Each possible random sample of 10 countries will produce its own <span style="color: red;"> sample mean or population estimate </span>.
```{r, echo = F}
sim_num <- seq(1:1000)
region_samp <- vector("list", 1000)
for (i in seq_along(region_samp)){
    region_samp[[i]] <- who %>% 
      filter(str_length(region) <= 12) %>% 
      sample_n(10)
}

names(region_samp) <- paste0("sim_", sim_num)
```

```{r, echo = F, fig.width = 8, fig.height = 5}
plots <- lapply(region_samp[1:4], function(x) {
  ggplot(x, aes(x = region, y = life_expectancy)) +
    geom_point() +
    geom_hline(
      yintercept = mean(x$life_expectancy),
      color = "red",
      linetype = "dotted"
    ) +
    geom_hline(yintercept = mean(who$life_expectancy),
               color = "blue") +
    geom_label(aes(
      x = +Inf,
      y = mean(x$life_expectancy),
      vjust = 2,
      label =  paste(mean(x$life_expectancy))
    ),
    colour = "red",
    size = 4) +
    ylim(50, 85) +
    coord_flip() +
    theme_minimal()
})
library(gridExtra)
marrangeGrob(plots, nrow=2, ncol=2, top = paste("Estimated Population Means Across Four Different Samples"))
```

These estimated means have their own variability around the true population mean.

---
# Population vs. Sample Distribution

If we sample our population **100 times** and plot the estimated mean of each sample, the estimated means begin to form their own normal distribution. The mean of sample means is our <span style="color: orange;"> **estimated population mean**. </span>.

```{r, echo = F}
samp_means <- tibble(samp = 1:1000, means = map_dbl(region_samp, ~mean(.$life_expectancy)))
```


```{r, animation.hook="gifski", interval = 0.4, echo = F, eval = T}
for (i in 1:100) {
  x <- ggplot(subset(samp_means, samp <= i), aes(means)) +
    geom_histogram(fill = "darkred", color = "grey") +
    geom_vline(xintercept = 71.64, color = "blue") +
    geom_vline(xintercept = mean(samp_means$means[samp_means$samp <= i]), color = "orange") +
    geom_label(aes(
      x = 71.64,
      y = +Inf,
      vjust = 2,
      label = 71.64
    ),
    colour = "blue",
    size = 4) +
        geom_label(aes(
      x = mean(samp_means$means[samp_means$samp <= i]),
      y = +Inf,
      vjust = 3,
      label = paste(round(mean(samp_means$means[samp_means$samp <= i]), 2))
    ),
    colour = "orange",
    size = 5) + 
    xlim(65, 85) +
    xlab("Mean Life Expectancy") +
    theme_minimal()
  print(x)
}

```

The mean of our sampling distribution (mean of means) approximates the <span style="color: blue;"> population mean</span>. This is an example of the **central limit theorem**.

If we sampled all possible samples of size *n* from our population, our mean of sample means would be equal to the population mean.

---
# Central Limit Theorem

> Given a population mean $\mu$ and standard deviation of $\sigma$, the sampling distribution of the mean is a normal distribution with a mean equal to $\mu$ and standard deviation equal to $\frac{\sigma}{\sqrt{n}}$, where *n* is the sample size.

> The distribution of sample means will approach the normal distribution as the sample size increases regardless of the shape of the population distribution.

* The sampling distribution of means approximates a normal distribution because it is estimating a statistic of the population distribution. It is not "recreating" the shape of the population distribution.

* Center: Mean of sample means is the population mean.
$$ \mu_\bar{X} = \mu$$

* Spread: Standard deviation of sample means is the standard error of the mean

$$ \sigma_\bar{X} = \frac{\sigma}{\sqrt{n}}$$

```{r, eval = F, echo = F}
for (i in seq(1, 100, 10)) {
 x <- ggplot(who, aes(life_expectancy)) +
     geom_histogram(fill = "darkred", color = "grey", bins = 30) +
     geom_vline(xintercept = 71.64, color = "blue") +
     geom_vline(xintercept = mean(samp_means$means[samp_means$samp <= i]), color = "green") +
     geom_label(aes(
       x = 71.64,
       y = +Inf,
       vjust = 2,
       label = 71.64
     ),
     colour = "blue",
     size = 4) +
     xlim(50, 85) +
   xlab("Mean Life Expectancy") +
     theme_minimal()
   print(x)
 }

```

---
# Central Limit Theorem

* Here is a histogram of the means of 1000 random samples from our WHO data.
* The center of the distribution is our population mean.
* The standard deviation is the standard error of the mean.
{https://onlinestatbook.com/stat_sim/sampling_dist/}

```{r, parse = T, echo = F}

ggplot(samp_means, aes(means)) +
  geom_histogram() +
  geom_vline(xintercept = mean(samp_means$means), color = "red") +
  geom_segment(
    x = mean(samp_means$means),
    xend = (mean(samp_means$means) + sd(samp_means$means)),
    y = 30,
    yend = 30,
    color = "red"
  ) +
  geom_label(aes(
    x = mean(samp_means$means),
    y = +Inf,
    vjust = 3,
    label = paste('mu', "==", round(mean(samp_means$means), 2))
  ), parse = T) +
    geom_label(aes(
    x = (mean(samp_means$means) + sd(samp_means$means)),
    y = +Inf,
    vjust = 6,
    label = paste('sigma', "==", round(sd(samp_means$means), 2))
  ), parse = T) +
  theme_classic()
  

```

---
# T-Tests
* One-Sample T-Test
* T-Distributions
* Applied Example
* Hypothesis Testing


---
# One Sample T-Test
* T-tests are one of the most foundational statistical tests in classical statistics.

* The purpose of a one-sample t-test is to describe the probability of obtaining a particular sample mean through random sampling variability, assuming the population mean = K.
  + e.g. Is our observed sample mean to be expected, assuming it reflects a random sample of the population?
  
\center **Null Hypothesis** $(H_0)$: $\mu = K$, where $K$ is any constant. \center

* Does **not** tests probability of obtaining the sample distribution or data itself, only the sample mean.

* Why is it called a t-test? 
  + We calculate probabilities from a t-distribution.
  
---
# T-Distributions
* In a z-test, the population standard deviation is assumed to be known.

* In practice, we **rarely** know the true population standard deviation due to:
  + Measurement Error
  + Population statistics can be in constant flux
  + Can be unfeasible to measure entire population

* T-Distributions:
  + Assume that the population standard deviation $(\sigma)$ is unknown. 
  + Resemble Z-distributions as degrees of freedom increase.
  + The degree of freedom for a t-test is always *n*-1, where *n* is your sample size.
  
```{r, echo = F}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm,
                aes(color = "normal")) +
  stat_function(fun = dt, 
                args = list(df = 5), 
                aes(color = "5")) +
  stat_function(fun = dt,
                args = list(df = 10),
                aes(color = "10")) +
  stat_function(fun = dt,
                args = list(df = 30),
                aes(color = "30")) +
  scale_colour_brewer("Degrees of Freedom", 
                      palette = "Set1",
                      limits = c("Normal", "5", "10", "30")) +
  ggtitle("T-Distributions") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))

```

---
# One-Sample T-Test

* The t-statistic is calculated like a z-score, but we replace the population standard deviation with the sample standard deviation.

.pull-right[
$$ t = \frac{\bar{X} - \mu}{\hat{\sigma}/\sqrt{n}} $$
]
.pull-left[
$$z = \frac{X - \mu}{\sigma}$$
]

* Distributional spread and proportion of the area under the curve beyond our t-value depends on our degrees of freedom.

* Using a t-test, we can calculate the probability of obtaining a particular sample mean through random sampling.


```{r , echo=F, animation.hook="gifski", interval=0.4}
samp_means <- tibble(df = seq(2, 20, 2), 
                     val = round(
                       map_dbl(seq(2, 20, 2), 
                                   ~ 2*pt(-abs(1), df = .x)),
                       digits = 3))

for (i in 1:10) {
  x <- ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = samp_means$df[i]), 
                color = "black") +
  stat_function(fun = dt, 
                args = list(df = samp_means$df[i]), 
                xlim = c(1,4),
                fill = "red",
                geom = "area") +
          geom_label(aes(
      x = 1,
      y = 0.3,
      label = "t = 1.0")) +
      geom_label(aes(
      x = 1.5,
      y = 0.04,
      label = samp_means$val[i])) +
  geom_vline(xintercept = 1) +
  ggtitle("T-Distribution", 
                subtitle = paste("Degrees of Freedom =", samp_means$df[i])) +
    xlab("T-Statistic")
  print(x)
}
```

---
## Applying a T-Test
Suppose we have data from a sample of 10 countries with a mean life expectancy of 66.87 and standard deviation of 5.82.

If our population mean is thought to be 78.16 $(H_0: \sigma = 78.16)$, what is the probability of obtaining a sample mean of 66.87, **or a more extreme mean value**, due to random sampling variability?
$$t(9) = \frac{66.87 - 75.16}{5.82/\sqrt{10}} = -1.24$$
.pull-left[
```{r, echo = F}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = 10), 
                color = "black") +
  stat_function(fun = dt, 
                args = list(df = 10), 
                xlim = c(-4, -1.24),
                fill = "red",
                geom = "area") +
  geom_vline(xintercept = -1.24) +
  geom_label(aes(
      x = -1.5,
      y = 0.04,
      label = paste(round(pt(q= -1.24, df = 10, lower.tail = T), 2)))) +
    xlab("T-Statistic") +
  ggtitle("One-tailed t-test",
          subtitle = paste("p = ", round(pt(q= -1.24, df = 10, lower.tail = T), 2)))

```
Assuming a population mean of 78.16 and sample size of 10, the proportion of random samples that would demonstrate a sample mean **equal to or lower than 66.87** is 0.12 (i.e., *p* = 0.12).
]

.pull-right[
```{r, echo = F}

ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = 9), 
                color = "black") +
  stat_function(fun = dt, 
                args = list(df = 9), 
                xlim = c(-4, -1.24),
                fill = "red",
                geom = "area") +
    stat_function(fun = dt, 
                args = list(df = 9), 
                xlim = c(1.24, 4),
                fill = "red",
                geom = "area") +
  geom_vline(xintercept = -1.24) +
   geom_label(aes(
      x = -1.5,
      y = 0.04,
      label = paste(round(pt(q= -1.24, df = 9, lower.tail = T), 2)))) +
   geom_label(aes(
      x = 1.5,
      y = 0.04,
      label = paste(round(pt(q= 1.24, df = 9, lower.tail = F), 2)))) +
    xlab("T-Statistic") +
  ggtitle("Two-tailed t-test",
          subtitle = paste("p =", 2*round(pt(q= -1.24, df = 9, lower.tail = T), 2)))

```
Assuming a population mean of 78.16 and sample size of 10, the proportion of random samples that would demonstrate a sample mean **equal to or more extreme than 66.87** is 0.24 (i.e., *p* = 0.24).
]

**Note:** One-tailed tests are rarely warranted and we will always be conducting two-tailed tests.

---
# *p*-Values and Alpha Thresholds
* Alpha $(\alpha)$ represents our Type 1 error rate:
  + Type 1 Error - Rejecting the null hypothesis when it is actually true
  + $\alpha = .05$ means we will mistakenly reject the null hypothesis 5% of the time.
  
* Alpha is also equal to the maximum p-value necessary to reject the null hypothesis.
  + If $\alpha = .05$ and $p < .05$, we can reject the null hypothesis.
  
* Alpha should be identified **prior to conducting a statistical test**.

* Critical t-values $t_{\alpha/2}(df)$ are the thresholds at which our alpha threshold is met.
  + Vary depending on $df$ and $\alpha$
  
```{r, echo = F, fig.width=5, fig.height=2}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = 10), 
                color = "black") +
  stat_function(fun = dt, 
                args = list(df = 10), 
                xlim = c(-4, -2.228),
                fill = "red",
                geom = "area") +
    stat_function(fun = dt, 
                args = list(df = 10), 
                xlim = c(2.228, 4),
                fill = "red",
                geom = "area") +
  geom_vline(xintercept = -2.228) +
  geom_vline(xintercept = 2.228) +
     geom_label(aes(
      x = -2.228,
      y = 0.2,
      label = paste("Critical T = -2.228"))) +
   geom_label(aes(
      x = 2.5,
      y = 0.2,
      label = paste("Critical T = 2.228"))) +
    xlab("T-Statistic") +
   geom_label(aes(
      x = -2.5,
      y = 0.04,
      label = paste(round(pt(q= -2.228, df = 10, lower.tail = T), 4)))) +
   geom_label(aes(
      x = 2.5,
      y = 0.04,
      label = paste(round(pt(q= 2.228, df = 10, lower.tail = F), 4)))) +
    xlab("T-Statistic") +
  ggtitle("Two-tailed t-test",
          subtitle = paste("p =", 2*round(pt(q= -2.228, df = 10, lower.tail = T), 4)))

```
With two-tailed tests, the total alpha is "split" across both tails of the distribution $(\alpha(.05)/2 = .025)$.
---
# Thinking about *p*-values

Assume that we find that average national life expectancy is significantly greater in high-income countries compared to low-income countries. With an $\alpha$ of .05, the (hypothetical) statistical test returns *p* = .03, meaning we can reject the null hypothesis.

Which of the following statements is correct, given *p* = .03:
* **A.** There is a 3% probability that high-income countries do not have a higher average life expectancy (the null hypothesis).
* **B.** There is a 3% probability that the results are due to random chance, rather than the effect of income level.
* **C.** A statistically significant difference means higher income-levels yield higher life expectancies.
* **D.** The observed data would only occur 3% of the time if the null hypothesis was true.
* **E.** Previous research found that in 2005, income-level failed to produce a significant difference in average life expectancy (*p* = .17). Therefore, income-level is more influential on life expectancy in 2015 than in 2005.
* **F.** Another research group finds a statistically significant difference between countries with socialized medicine compared to those without (*p* = .001). Their smaller *p* value means that socialized medicine is more effective at increasing life expectancy than income level.

---
# Thinking about *p*-values

* None of the previous interpretations were correct, yet these are commonly expressed in scientific literature!

* In frequentist statistics, we are accepting or rejecting the null hypothesis, set by a particular alpha value. 

* P-values are not a gradient and should only reference the alpha threshold and the null hypothesis.

---
# Analogy
### (Feel free to drop or edit this analogy)

You are given a six-sided die and told it is normally weighted. 

$H_0:$ The die is normally weighted.

If the die is normally weighted, each number should come up with approximately equal frequency across 1000 rolls. Depending on the outcome and its probability under the null hypothesis, you can accept or reject the null hypothesis.

An outcome can be more or less unlikely with a equally weighted die, but the die does not"approach unequal weighting" or is not a "marginally loaded die."

---

  

