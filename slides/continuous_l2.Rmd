---
title: "EDUC 641 Continuous Data"
author: "Professor Name"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
  html_document:
    keep_md: true
---
class: inverse, center, middle

```{r setup, include = FALSE}
options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(warning = FALSE,
                      message = FALSE,
                      echo = FALSE,
                      fig.align = "center",
                      fig.height = 3)

if (!require(pacman)) install.packages('pacman', repos = 'https://cran.rstudio.com')
pacman::p_load(tidyverse, here, knitr, dplyr, ggplot2, blogdown, rio, gifski, palmerpenguins, xaringanthemer)

extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".red-pink" = list(color= "red_pink"),
  ".grey-light" = list(color= "grey_light"),
  ".purple" = list(color = "purple"),
  ".small" = list("font-size" = "90%"))

write_extra_css(css = extra_css, outfile = "my_custom.css")

```

```{r, echo = F}
who <- import(here::here("data", "life_expectancy.csv")) %>% 
  janitor::clean_names() %>% 
  filter(year == 2015) %>%
  select(country, status, year, life_expectancy) %>% 
  rename(region = country) %>% 
  mutate(life_expectancy = round(life_expectancy, digits = 0))

```

---
# Normal Distributions

The normal distribution describes a frequently occurring phenomenon across the social and natural sciences.

```{r}
ggplot(subset(penguins, species == "Adelie"), aes(flipper_length_mm)) +
  geom_density() + 
  ggtitle("Flipper Length of Adelie Penguins") +
  xlab("Flipper Length (mm)") +
  theme_minimal()

birthweight <- import(here::here("data", "birthweight.csv"))

ggplot(birthweight, aes(Birthweight)) +
  geom_density() +
  ggtitle("Human Birthweight") +
  xlab("Birthweight (kg)") +
  theme_minimal()
```

---
# Normal Distributions
The normal distribution is a mathematical function that replicates this natural phenomenon using two parameters:
* Mean $(\mu)$
* Standard Deviation $(sigma)$

It is defined by the following probability density function: 
$$ p(X|\mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma}}exp[-\frac{(X-\mu)^2}{2\sigma^2}] $$
$$X\sim N(\mu,\sigma)$$
**The normal curve is only a model of patterns that exist in the real world.**

> "All models are wrong, but some are useful." - George Box

---
# Normal Distributions

A normal distribution is a **theoretical** distribution with the following characteristics:

Central Tendency:
+ The mean, median, and mode are equivalent values.

Variability:
+ 68% of observations are within 1 SD
+ 95% of  observations are within 2 SDs
+ 99% of observations are within 3 SDs

Shape: 
+ Unimodal
+ Symmetrical
+ Skewness & Kurtosis = 0

---

# Normal Distributions

* The area under a normal distribution is always equal to 100% (it represents the whole of observations)

* The area under the normal distribution curve corresponds to the percentage of observations.

* We can always infer the percentage of observations in any part of a normal distribution.

```{r, echo = F}
sim_data <- data_frame(value = rnorm(n = 100000, mean = 0, sd = 1))

ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm) +
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = .38)) +
  geom_segment(aes(x = -1, xend = -1, y = 0, yend = .24)) +
  geom_segment(aes(x = 1, xend = 1, y = 0, yend = .24)) +
  geom_segment(aes(x = -2, xend = -2, y = 0, yend = .056)) +
  geom_segment(aes(x = 2, xend = 2, y = 0, yend = .056)) +
  geom_segment(aes(x = -3, xend = -3, y = 0, yend = .005)) +
  geom_segment(aes(x = 3, xend = 3, y = 0, yend = .005)) +
  annotate("text", x = -.5, y = .2, label = "34%") +
  annotate("text", x = .5, y = .2, label = "34%") +
  annotate("text", x = -1.5, y = .05, label = "14%") +
  annotate("text", x = 1.5, y = .05, label = "14%") +
  annotate("text", x = -2.5, y = .05, label = "2%") +
  annotate("text", x = 2.5, y = .05, label = "2%") +
  annotate("text", x = -3.5, y = .05, label = "<1%") +
  annotate("text", x = 3.5, y = .05, label = "<1%") +
  scale_x_continuous(limits = c(-4, 4),
                     breaks = c(seq(-4, 4, 1))) +
  ylim(0, 0.4) +
  theme_classic()

```

---

# Empirical Rule
We can use our understanding of the normal distribution to calculate the probability of observing a value in a certain range. This is known as the **empirical rule**.

.pull-left[
A randomly selected observation has an approximately...
* **68% chance of being within 1 SD** of the mean.
 + $P(-1 < Z < 1) = 0.683$
]
.pull-right[
```{r}
emp_rule <- ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm) +
  stat_function(fun = dnorm,
                xlim = c(-1, 1),
                fill = "purple",
                geom = "area") +
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = .38)) +
  geom_segment(aes(x = -1, xend = -1, y = 0, yend = .24)) +
  geom_segment(aes(x = 1, xend = 1, y = 0, yend = .24)) +
  annotate("text", x = -.5, y = .2, label = "34%") +
  annotate("text", x = .5, y = .2, label = "34%") +
  geom_label(x = 0, y = 0.1, label = "68%") +
  scale_x_continuous(limits = c(-4, 4),
                     breaks = c(seq(-4, 4, 1))) +
  ylim(0, 0.4) +
  theme_classic()

```
]
 
---
# Empirical Rule

We can use our understanding of the normal distribution to calculate the probability of observing a value in a certain range. This is known as the **empirical rule**.

.pull-left[
A randomly selected observation has an approximately...
* **68% chance of being within 1 SD** of the mean.
 + $P(-1 < Z < 1) = 0.683$
* **95.5% chance of being within 2 SD** of the mean.
 + $P(-2 < Z < 2) = 0.955$
]

.pull-right[
```{r}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm) +
    stat_function(fun = dnorm,
                xlim = c(-2, 2),
                fill = "purple",
                geom = "area") +
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = .38)) +
  geom_segment(aes(x = -1, xend = -1, y = 0, yend = .24)) +
  geom_segment(aes(x = 1, xend = 1, y = 0, yend = .24)) +
  annotate("text", x = -.5, y = .2, label = "34%") +
  annotate("text", x = .5, y = .2, label = "34%") +
    annotate("text", x = -1.5, y = .05, label = "13.5%") +
  annotate("text", x = 1.5, y = .05, label = "13.5%") +
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = .38)) +
  geom_segment(aes(x = -2, xend = -2, y = 0, yend = .056)) +
  geom_segment(aes(x = 2, xend = 2, y = 0, yend = .056)) +
  geom_label(x = 0, y = 0.1, label = "95.5%") +
  scale_x_continuous(limits = c(-4, 4),
                     breaks = c(seq(-4, 4, 1))) +
  ylim(0, 0.4) +
  theme_classic()
```
]

---
# Empirical Rule

We can use our understanding of the normal distribution to calculate the probability of observing a value in a certain range. This is known as the **empirical rule**.

.pull-left[
A randomly selected observation has an approximately...
* **68% chance of being within 1 SD** of the mean.
 + $P(-1 < Z < 1) = 0.683$
* **95.5% chance of being within 2 SD** of the mean.
 + $P(-2 < Z < 2) = 0.955$
* **99.7% chance of being within 3 SD** of the mean.
 +  $P(-3 < Z < 3) = 0.997$
]

.pull-right[
```{r}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm) +
    stat_function(fun = dnorm,
                xlim = c(-3, 3),
                fill = "purple",
                geom = "area") +
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = .38)) +
  geom_segment(aes(x = -1, xend = -1, y = 0, yend = .24)) +
  geom_segment(aes(x = 1, xend = 1, y = 0, yend = .24)) +
  annotate("text", x = -.5, y = .2, label = "34%") +
  annotate("text", x = .5, y = .2, label = "34%") +
    annotate("text", x = -1.5, y = .05, label = "13.5%") +
  annotate("text", x = 1.5, y = .05, label = "13.5%") +
      annotate("text", x = -2.6, y = .04, label = "0.15%") +
  annotate("text", x = 2.6, y = .04, label = "0.15%") +
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = .38)) +
  geom_segment(aes(x = -2, xend = -2, y = 0, yend = .056)) +
  geom_segment(aes(x = 2, xend = 2, y = 0, yend = .056)) +
    geom_segment(aes(x = -3, xend = -3, y = 0, yend = .005)) +
  geom_segment(aes(x = 3, xend = 3, y = 0, yend = .005)) +
  geom_label(x = 0, y = 0.1, label = "99.7%") +
  scale_x_continuous(limits = c(-4, 4),
                     breaks = c(seq(-4, 4, 1))) +
  ylim(0, 0.4) +
  theme_classic()

```
]
---
# What is normal?

* A normal distribution can take on any mean and standard deviation.

```{r}
ggplot(data.frame(x = c(-20, 20)), aes(x = x)) +
  stat_function(fun = dnorm, args = c(mean = 10, sd = 1), color = "red") +
    stat_function(fun = dnorm, args = c(mean = 3, sd = 3), color = "black") +
    stat_function(fun = dnorm, args = c(mean = -2, sd = 7), color = "blue") +
  ggtitle("Normal Distributions") +
  theme_classic()

```
* Regardless of the mean and standard deviation, the area under the curve is always equal to 1.
* The empirical rule still applies (e.g., 68% of observations occur within 1 SD of the mean).
---

# Standard Normal Distribution

* The **standard** normal distribution has a mean of 0 and a standard deviation of 1.
  
```{r}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm) +
  theme_classic()

```

* The x-axis represents z-scores, meaning an observation's value represents how far it is from the mean.

  + A z-score of -1 indicates that an observation is 1 SD below the mean.
  + A z-score of +2 indicates that an observation is 2 SD above the mean.

* With a standard normal distribution, we can easily calculate the percentage of observations under any region.

---
# Area Under the Normal Curve
Suppose we wanted to know the probability of scoring at least 1.5 SD above the mean on a standardized test of academic achievement. 

```{r}
ztest_plot <- ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm) +
  geom_vline(xintercept = 1.5, color = "purple") +
  stat_function(fun = dnorm,
                xlim = c(1.5, 4),
                fill = "purple",
                geom = "area") +
  theme_classic()

ztest_plot
```
To answer this question, we just need to find the area of the shaded portion under the curve.

---
# Area Under the Normal Curve
* We can either use a computational tool or a z-score look-up table to calculate the area under the normal curve.

```{r}
ztest_plot +
    geom_label(x = 1.8, y = .02, label = ".07")

```
--
Either approach would show the area above 1.5 SD of a standard normal curve is .07.

--
Therefore, the probability of scoring above 1.5 SD on a standardized test is 7%.

---
# Z-Test
Let's try it with our WHO data:
* Suppose that the population mean national life expectancy is 71.64 $(\mu)$ with a SD of 8.15 $(\sigma)$.

* **Assuming a normal distribution, what is the probability of randomly selecting a country with a life expectancy greater than 76?**

* First, we need to standardize our value before we can calculate the percentage of observations above our z-score.

$$ z = \frac{76-71.64}{8.15} = 0.535$$

```{r}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm) +
  stat_function(fun = dnorm,
                xlim = c(0.53, 4),
                fill = "purple",
                geom = "area") +
  geom_vline(xintercept = 0.53) +
  geom_label(aes(
    x = 0.53,
    y = +Inf,
    vjust = 3,
    label = paste("z = 0.53"))) +
  scale_x_continuous(limits = c(-4, 4),
                     breaks = c(seq(-4, 4, 1))) +
  ylim(0, 0.4) +
  theme_minimal()
```


---
# Z-Test

```{r}
z.test = function(x,mu,popvar){

one.tail.p <- NULL

z.score <- round((mean(x)-mu)/(popvar/sqrt(length(x))),3)

one.tail.p <- round(pnorm(abs(z.score),lower.tail = FALSE),3)

cat(" z =",z.score,"\n",

"one-tailed probability =", one.tail.p,"\n",

"two-tailed probability =", 2*one.tail.p )}

z.test(x = 76, mu = 71.64, popvar = 8.15)
```

The area under the curve above a z-score of 0.53 is .30. Therefore, the probability of randomly selecting a country with a national life expectancy greater than 76 is 30%.

```{r}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm) +
  stat_function(fun = dnorm,
                xlim = c(0.53, 4),
                fill = "purple",
                geom = "area") +
  geom_vline(xintercept = 0.53) +
  geom_label(aes(
    x = 0.53,
    y = +Inf,
    vjust = 3,
    label = paste("z = 0.53"))) +
  annotate("text", x = 1.25, y = .05, label = "30%") +
  scale_x_continuous(limits = c(-4, 4),
                     breaks = c(seq(-4, 4, 1))) +
  ylim(0, 0.4) +
  theme_classic()
```

---
# Population vs. Sample Distributions
 
Using the WHO dataset, let's say we want to identify the mean life expectancy across all countries. However, let's also assume we only have the resources to sample from 10 countries.

.pull-left[

Our **population statistic** represents the true value across all countries.

```{r, echo = F}
world <- map_data("world")

regions <- world %>% 
  distinct(region) %>% 
  sample_n(10) %>% 
  mutate(sample = 1)

world <- left_join(world, regions) %>% 
  mutate(population = 1)
```

```{r, echo = F, fig.height=3, fig.width= 4}
ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region, fill = as.factor(population)),
    color = "black", size = 0.1
  ) +
  theme(legend.position = "none")
```
]

.pull-right[
Our **sample statistic** will be our estimate of the population statistic using the countries we examine.

```{r, echo = F, fig.height = 3, fig.width = 4}
ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region, fill = as.factor(sample)),
    color = "black", size = 0.1
  ) +
  theme(legend.position = "none")
```
]

---
# Sample Estimates

* Assume that our <span style="color: blue;">true population mean $(\mu)$ </span> is 71.64. 

* Each possible random sample of 10 countries will produce its own <span style="color: red;"> sample mean $(\bar{x})$ or population mean estimate </span>.

* These estimated means have their own variability around the true population mean.

```{r, echo = F}
sim_num <- seq(1:1000)
region_samp <- vector("list", 1000)
for (i in seq_along(region_samp)){
    region_samp[[i]] <- who %>% 
      filter(str_length(region) <= 12) %>% 
      sample_n(10)
}

names(region_samp) <- paste0("sim_", sim_num)
```

```{r, echo = F, fig.width = 8, fig.height = 5}
plots <- lapply(region_samp[1:4], function(x) {
  ggplot(x, aes(x = region, y = life_expectancy)) +
    geom_point() +
    geom_hline(
      yintercept = mean(x$life_expectancy),
      color = "red",
      linetype = "dotted"
    ) +
    geom_hline(yintercept = mean(who$life_expectancy),
               color = "blue") +
    geom_label(aes(
      x = +Inf,
      y = mean(x$life_expectancy),
      vjust = 2,
      label =  paste(mean(x$life_expectancy))
    ),
    colour = "red",
    size = 4) +
    ylim(50, 85) +
    coord_flip() +
    theme_minimal()
})
library(gridExtra)
marrangeGrob(plots, nrow=2, ncol=2, top = paste("Estimated Population Means Across Four Different Samples"))
```

---
# Population vs. Sample Distribution

If we sample our population **100 times** and plot the estimated mean of each sample, the estimated means begin to form their own normal distribution. The mean of sample means is our <span style="color: orange;"> **estimated population mean** $(\hat{\mu})$ </span>.

```{r, echo = F}
samp_means <- tibble(samp = 1:1000, means = map_dbl(region_samp, ~mean(.$life_expectancy)))
```


```{r, animation.hook="gifski", interval = 0.4, echo = F, eval = T}
for (i in 1:10) {
  x <- ggplot(subset(samp_means, samp <= i), aes(means)) +
    geom_histogram(fill = "orange", color = "grey") +
    geom_vline(xintercept = 71.64, color = "blue") +
    geom_vline(xintercept = mean(samp_means$means[samp_means$samp <= i]), color = "orange") +
    geom_label(aes(
      x = 71.64,
      y = +Inf,
      vjust = 2,
      label = 71.64
    ),
    colour = "blue",
    size = 4) +
        geom_label(aes(
      x = mean(samp_means$means[samp_means$samp <= i]),
      y = +Inf,
      vjust = 3.5,
      label = paste(round(mean(samp_means$means[samp_means$samp <= i]), 2))
    ),
    colour = "orange",
    size = 4) + 
    xlim(65, 85) +
    xlab("Mean Life Expectancy") +
    theme_minimal()
  print(x)
}

```

The mean of our sampling distribution (mean of means) approaches the <span style="color: blue;"> population mean $({\mu})$</span>. This is an example of the **central limit theorem**.

If we sampled all possible samples of size *n* from our population, our mean of sample means would be equal to the population mean.

---
## Central Limit Theorem

* There is a difference between the fact that many populations approximate the normal distribution vs. the normal curve of sampling distributions.

* Random sampling from the population will return means that will be asymptotically normal in their distribution as the number of samples approaches infinity.

* Because of that mathematical truth, we can conduct inference in the statistics.

--

.pull-left[
* Here is a histogram of the means of 1000 random samples from our WHO data.
 + The center of the distribution is our population mean.
 + The standard deviation is the standard error of the mean.
]

.pull-right[
```{r, parse = T, echo = F, fig.width=5, fig.height = 3}
ggplot(samp_means, aes(means)) +
  geom_histogram() +
  geom_vline(xintercept = mean(samp_means$means), color = "red") +
  geom_segment(
    x = mean(samp_means$means),
    xend = (mean(samp_means$means) + sd(samp_means$means)),
    y = 30,
    yend = 30,
    color = "red"
  ) +
  geom_label(aes(
    x = mean(samp_means$means),
    y = +Inf,
    vjust = 3,
    label = paste('mu', "==", 71.64)
  ), parse = T) +
    geom_label(aes(
    x = (mean(samp_means$means) + sd(samp_means$means)),
    y = +Inf,
    vjust = 6,
    label = paste('sigma', "==", round(sd(samp_means$means), 2))
  ), parse = T) +
  theme_classic()
```
]

Even though our life expectancy distribution is only approximately normal, our sampling curve **is** normal.

---
## Central Limit Demonstration

See the following the link for more demonstrations of the central limit theorem:
{https://onlinestatbook.com/stat_sim/sampling_dist/ }

---
# Central Limit Theorem

> Given a population mean $\mu$ and standard deviation of $\sigma$, the sampling distribution of the mean is a normal distribution with a mean equal to $\mu$ and standard deviation equal to $\frac{\sigma}{\sqrt{n}}$, where *n* is the sample size.

> The distribution of sample means will approach the normal distribution as the sample size increases regardless of the shape of the population distribution.

* The sampling distribution of means approximates a normal distribution because it is estimating a statistic of the population distribution. It is not "recreating" the shape of the population distribution.

* Center: Mean of sample means is the population mean.
$$ \mu_\bar{X} = \mu$$

* Spread: Standard deviation of sample means is the standard error of the mean

$$ \sigma_\bar{X} = \frac{\sigma}{\sqrt{n}}$$

```{r, eval = F, echo = F}
for (i in seq(1, 100, 10)) {
 x <- ggplot(who, aes(life_expectancy)) +
     geom_histogram(fill = "darkred", color = "grey", bins = 30) +
     geom_vline(xintercept = 71.64, color = "blue") +
     geom_vline(xintercept = mean(samp_means$means[samp_means$samp <= i]), color = "green") +
     geom_label(aes(
       x = 71.64,
       y = +Inf,
       vjust = 2,
       label = 71.64
     ),
     colour = "blue",
     size = 4) +
     xlim(50, 85) +
   xlab("Mean Life Expectancy") +
     theme_minimal()
   print(x)
 }

```

---
# T-Tests
* One-Sample T-Test
* T-Distributions
* Applied Example
* Hypothesis Testing

---
# Population Variance
* In a z-test, the population standard deviation is assumed to be known.

* In practice, we **rarely** know the true population standard deviation due to:
  + Measurement Error
  + Population statistics can be in constant flux (e.g., average life expectancies can change moment to moment)
  + Can be unfeasible to measure entire population

---
# T-Distributions
* Assume that the population standard deviation $(\sigma)$ is unknown. 
* T-distributions with smaller degrees of freedom have fatter tails. Assumes more uncertainty with smaller sample sizes.
* Resemble Z-distributions as degrees of freedom increase.
* The degrees of freedom for a t-test is always *n*-1, where *n* is your sample size.
  
```{r, echo=F, animation.hook="gifski", interval=0.4,}

df_seq <- tibble(df = seq(1, 30, 3))

for (i in 1:nrow(df_seq)) {
x <- ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm,
                aes(color = "normal")) +
  stat_function(fun = dt, 
                args = list(df = df_seq$df[i]),
                color = "green") +
  ggtitle("T-Distributions",
          subtitle = paste("df =", df_seq$df[i])) +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))
print(x)
}
```


```{r , echo=F, animation.hook="gifski", interval=0.4, eval = F}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm,
                aes(color = "normal")) +
  stat_function(fun = dt, 
                args = list(df = 5), 
                aes(color = "5")) +
  stat_function(fun = dt,
                args = list(df = 10),
                aes(color = "10")) +
  stat_function(fun = dt,
                args = list(df = 30),
                aes(color = "30")) +
  scale_colour_brewer("Degrees of Freedom", 
                      palette = "Set1",
                      limits = c("Normal", "5", "10", "30")) +
  ggtitle("T-Distributions") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))


```

---
# One Sample T-Test
* T-tests are one of the most foundational statistical tests in classical statistics.

* The purpose of a one-sample t-test is to describe the probability of obtaining a particular sample mean through random sampling variability, assuming the population mean = K.
  + e.g. Is our observed sample mean to be expected if we repeatedly drew random samples from the population?
  
  
**Example Research Question:**
Suppose the known average life expectancy is 76 years with a standard deviation of 3.2. Do countries around the Mediterranean Sea significantly differ from the known population average of 76?


---
# One Sample T-Test
*  The one sample t-test does **not** test the probability of obtaining the sample distribution or data itself, only the sample mean.
  
*  Using the probability of obtaining a particular sample mean **Null Hypothesis** $(H_0)$: $\mu = K$, where $K$ is any constant.


---
# One-Sample T-Test
* With a z-test, we were able to find the number of observations that fell beyond a particular value. That value was our z-statistic.
* The t-statistic is functionally like a z-statistic, but for the t-distribution.

```{r}
z_dist <- ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm,
                args = list(mean = 0, sd = 1)) +
  geom_vline(xintercept = 1) +
  geom_label(x = 1, y = 0.25, label = "z-statistic") +
  xlab("z-score") +
  ggtitle("Standard Normal Distribution") +
  theme_classic()



t_dist <- ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt,
                args = list(df = 10)) +
  geom_vline(xintercept = 1) +
  geom_label(x = 1, y = 0.25, label = "t-statistic") +
  xlab("t-value") +
  ggtitle("T-Distribution (df = 10)") +
  theme_classic()

gridExtra::grid.arrange(z_dist, t_dist, ncol = 2)
```


---
# T-Distribution
* Shape of the t-distribution is slightly different depending on the degrees of freedom $(n-1)$.

* Thus, the proportion of the area under the curve beyond our t-value also depends on our degrees of freedom.


```{r , echo=F, animation.hook="gifski", interval=0.4}
samp_means <- tibble(df = seq(2, 20, 2), 
                     val = round(
                       map_dbl(seq(2, 20, 2), 
                                   ~ 2*pt(-abs(1), df = .x)),
                       digits = 3))

for (i in 1:10) {
  x <- ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = samp_means$df[i]), 
                color = "black") +
  stat_function(fun = dt, 
                args = list(df = samp_means$df[i]), 
                xlim = c(1,4),
                fill = "red",
                geom = "area") +
          geom_label(aes(
      x = 1,
      y = 0.3,
      label = "t = 1.0")) +
      geom_label(aes(
      x = 1.5,
      y = 0.04,
      label = samp_means$val[i])) +
  geom_vline(xintercept = 1) +
  ggtitle("T-Distribution", 
                subtitle = paste("Degrees of Freedom =", samp_means$df[i])) +
    xlab("T-Statistic")
  print(x)
}
```

---
# One-Sample T-Test

* To calculate a t-statistic, we use the same formula to calculate a z-score, but replace the population standard deviation with the sample's standard deviation.

Since we don't know the population standard deviation $(\mu)$, our samples standard deviation $(s)$ is used as an estimate of the population standard deviation $(\hat{\mu})$.

$$ t = \frac{\bar{X} - \mu}{s/\sqrt{n}} $$

---
## Applying a t-test
Suppose we have data from a sample of 10 countries with a mean life expectancy of 66.87 and standard deviation of 5.82.

If our population mean is thought to be 78.16 $(H_0: \sigma = 78.16)$, what is the probability of obtaining a sample mean of 66.87, **or a more extreme mean value**, due to random sampling variability?
$$t(9) = \frac{66.87 - 75.16}{5.82/\sqrt{10}} = -1.24$$
```{r}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = 9), 
                color = "black") +
  geom_vline(xintercept = -1.24, color = "red") +
  geom_label(aes(
      x = -1.24,
      y = 0.2,
      label = paste("t = -1.24"))) +
    xlab("T-Statistic") +
  theme_minimal()

```

---
# Applying a t-test

Assuming a population mean of 78.16 and sample size of 10...

.pull-left[
```{r, echo = F}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = 9), 
                color = "black") +
  stat_function(fun = dt, 
                args = list(df = 9), 
                xlim = c(-4, -1.24),
                fill = "red",
                geom = "area") +
  geom_vline(xintercept = -1.24) +
  geom_label(aes(
      x = -1.5,
      y = 0.04,
      label = paste(round(pt(q= -1.24, df = 9, lower.tail = T), 2)))) +
    xlab("T-Statistic") +
  theme_minimal() +
  ggtitle("One-tailed t-test",
          subtitle = paste("p = ", round(pt(q= -1.24, df = 9, lower.tail = T), 2))) +
    theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

```

...the proportion of random samples that would demonstrate a sample mean **equal to or lower than 66.87** is 0.12 (i.e., *p* = 0.12).
]

.pull-right[
```{r, echo = F}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = 9), 
                color = "black") +
  stat_function(fun = dt, 
                args = list(df = 9), 
                xlim = c(-4, -1.24),
                fill = "red",
                geom = "area") +
    stat_function(fun = dt, 
                args = list(df = 9), 
                xlim = c(1.24, 4),
                fill = "red",
                geom = "area") +
  geom_vline(xintercept = -1.24) +
   geom_label(aes(
      x = -1.5,
      y = 0.04,
      label = paste(round(pt(q= -1.24, df = 9, lower.tail = T), 2)))) +
   geom_label(aes(
      x = 1.5,
      y = 0.04,
      label = paste(round(pt(q= 1.24, df = 9, lower.tail = F), 2)))) +
    xlab("T-Statistic") +
  theme_minimal() +
  ggtitle("Two-tailed t-test",
          subtitle = paste("p =", 2*round(pt(q= -1.24, df = 9, lower.tail = T), 2))) +
    theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))
```
...the proportion of random samples that would demonstrate a sample mean **equal to or more extreme than 66.87** is 0.24 (i.e., *p* = 0.24).
]

**Note:** One-tailed tests are rarely warranted and we will always be conducting two-tailed tests.

> If the null hypothesis is true, the probability of obtaining a sample mean equal to or more extreme 66.87 is 0.24.

---
## One-Tailed Test vs. Two Tailed Test

A one vs. two tailed test apply different alternative hypotheses from the null.

$$H_0: \mu = K$$
.pull-left[
One-Tailed

* $H_A: \mu < K$ or $\mu > K$

Tests for a significant difference in one direction. Assumes a priori that the difference can only occur in one direction.

]

.pull-right[
Two-Tailed

* $H_A: \mu \neq K$

Tests the possibility of a significant difference in either direction.
]

---
# Demonstration

* Dr. DSM and Dr. APA have developed a new intervention for depression.

* They would like to compare the post-treatment depression ratings for their research participants to average depression ratings following "business as usual."

* They conduct a one-tailed t-test to see if their sample's mean post-treatment rating is significantly better than the normal post-treatment averages.
 + $(H_0: \mu < K)$
 + Lower scores depression ratings are desirable.

---
# Demonstration

Dr. DSM finds that the mean post-intervention depression rating did not fall below the critical t-value for the one-tailed test. The new intervention group did not demonstrate significantly lower than normal post-treatment averages.
```{r, echo = F}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = 30), 
                color = "black") +
  stat_function(fun = dt, 
                args = list(df = 30), 
                xlim = c(-4, -1.70),
                fill = "red",
                geom = "area") +
  geom_vline(xintercept = 3.8) + 
  geom_label(x = 3.8, y = 0.2, label = "Sample Mean", size = 2) +
  theme_minimal()

```

What conclusion was missed with the one-tailed test?

--
The sample had much higher levels of depression than we'd expect! Neither the null $(\mu = K)$ or alternative hypothesis $(\mu < K)$ seem tenable.

---
# Alpha Thresholds and t-values

With two-tailed tests, the total alpha is "split" across both tails of the distribution.
 * If $\alpha = .05$, $\alpha/2 = 0.025$.

* Critical t-values $t_{\alpha/2}(df)$ are the values at which our alpha threshold is met.
  
```{r, echo = F}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = 9), 
                color = "black") +
  stat_function(fun = dt, 
                args = list(df = 9), 
                xlim = c(-4, -2.2622),
                fill = "red",
                geom = "area") +
    stat_function(fun = dt, 
                args = list(df = 9), 
                xlim = c(2.2622, 4),
                fill = "red",
                geom = "area") +
  geom_vline(xintercept = -2.2622) +
  geom_vline(xintercept = 2.2622) +
     geom_label(aes(
      x = -2.2622,
      y = 0.2,
      label = paste("Critical T = -2.2622"))) +
   geom_label(aes(
      x = 2.5,
      y = 0.2,
      label = paste("Critical T = 2.2622"))) +
    xlab("T-Statistic") +
   geom_label(aes(
      x = -2.5,
      y = 0.04,
      label = paste(round(pt(q= -2.2622, df = 9, lower.tail = T), 4)))) +
   geom_label(aes(
      x = 2.5,
      y = 0.04,
      label = paste(round(pt(q= 2.2622, df = 9, lower.tail = F), 4)))) +
    xlab("T-Statistic") +
  theme_minimal() +
  ggtitle("Two-tailed t-test",
          subtitle = paste("p =", 2*round(pt(q= -2.2622, df = 9, lower.tail = T), 4)))
```

If we conducted a t-test, our t-statistic would have to be less than -2.2622 or greater than 2.2622 to reject the null hypothesis.

---
# One Sample T-test

* Assuming we had set our $\alpha$ at .05, let's see if we reject our null hypothesis in our previous example.

.pull-left[
Our observed t-value did not surpass our critical t-values.
```{r}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = 9), 
                color = "black") +
  geom_vline(xintercept = -1.24, color = "red") +
  geom_label(aes(x = -1.24, y = 0.3, 
                 label = "Observed T")) +
  geom_vline(xintercept = -2.2622) +
  geom_vline(xintercept = 2.2622) +
     geom_label(aes(
      x = -2.2622,
      y = 0.2,
      label = paste("Critical T"))) +
   geom_label(aes(
      x = 2.5,
      y = 0.2,
      label = paste("Critical T"))) +
    xlab("T-Statistic") +
  theme_minimal() +
  ggtitle("Two-tailed t-test",
          subtitle = paste("p =", 2*round(pt(q= -1.24, df = 9, lower.tail = T), 2))) +
    theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

```
]

.pull-right[
Our p-value is not less than .05 and does not satisfy our alpha threshold.
```{r}

two_tailed_test <- ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = 9), 
                color = "black") +
  stat_function(fun = dt, 
                args = list(df = 9), 
                xlim = c(-4, -1.24),
                fill = "red",
                geom = "area") +
    stat_function(fun = dt, 
                args = list(df = 9), 
                xlim = c(1.24, 4),
                fill = "red",
                geom = "area") +
  geom_vline(xintercept = -1.24) +
   geom_label(aes(
      x = -1.5,
      y = 0.04,
      label = paste(round(pt(q= -1.24, df = 9, lower.tail = T), 2)))) +
   geom_label(aes(
      x = 1.5,
      y = 0.04,
      label = paste(round(pt(q= 1.24, df = 9, lower.tail = F), 2)))) +
    xlab("T-Statistic") +
  theme_minimal() +
  ggtitle("Two-tailed t-test",
          subtitle = paste("p =", 2*round(pt(q= -1.24, df = 9, lower.tail = T), 2))) +
    theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

two_tailed_test
```
]

Whether we compare our t-statistic to our critical t-values or our look at our p-value, we come to the same conclusion.


---
# One Sample T-test

```{r}
two_tailed_test
```

Assuming a population mean of 78.16 and sample size of 10, the proportion of random samples that would demonstrate a sample mean **equal to or more extreme than 66.87** is 0.24 $(t = -1.24, p = 0.24)$. Because our p-value does not meet our alpha threshold of .05, we **fail to reject the null hypothesis**.

Thus, we can conclude that the mean life expectancy of this sample of countries is not different from the mean population average.

