---
title: "EDUC 641 Continuous Data"
author: "Professor Name"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    css: ['default','new.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
  html_document:
    keep_md: true
---
class: inverse, center, middle

```{r setup, include = FALSE}
options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(warning = FALSE,
                      message = FALSE,
                      echo = FALSE,
                      fig.align = "center",
                      fig.height = 3)

if (!require(pacman)) install.packages('pacman', repos = 'https://cran.rstudio.com')
pacman::p_load(tidyverse, here, knitr, dplyr, ggplot2, blogdown, rio, gifski)
```

```{r, echo = F}
who <- import(here::here("data", "life_expectancy.csv")) %>% 
  janitor::clean_names() %>% 
  filter(year == 2015) %>%
  select(country, status, year, life_expectancy) %>% 
  rename(region = country) %>% 
  mutate(life_expectancy = round(life_expectancy, digits = 0))

needs(car)
leveneTest(life_expectancy ~ status, data = who)

```

# Agenda
* Sampling Distributions
* Central Limit Theorem



---
#Noraml Distributions

The normal distribution is a frequently occurring phenomenon across the social and natural sciences.

Show examples

We can replicate this phenomenon using a mathematical function.

law of math, "not real" or truth. we can generate a function that resembles the natural phenomenon found in the real world. 

"Model" this natural phenomenon. 
---
# Normal Distributions

A normal distribution is a **theoretical** distribution with the following characteristics:

Central Tendency:
+ The mean, median, and mode are equivalent values.

Variability:
+ 68% of observations are within 1 SD
+ 95% of  observations are within 2 SDs
+ 99% of observations are within 3 SDs

Shape: 
+ Unimodal
+ Symmetrical
+ Skewness & Kurtosis = 0

---

# Normal Distributions

* The area under a normal distribution is always equal to 100% (it represents the whole of observations)

* The area under the normal distribution curve corresponds to the percentage of observations.

* We can always infer the percentage of observations in any part of a normal distribution.

```{r, echo = F}
sim_data <- data_frame(value = rnorm(n = 100000, mean = 0, sd = 1))

ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm) +
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = .38)) +
  geom_segment(aes(x = -1, xend = -1, y = 0, yend = .24)) +
  geom_segment(aes(x = 1, xend = 1, y = 0, yend = .24)) +
  geom_segment(aes(x = -2, xend = -2, y = 0, yend = .056)) +
  geom_segment(aes(x = 2, xend = 2, y = 0, yend = .056)) +
  geom_segment(aes(x = -3, xend = -3, y = 0, yend = .005)) +
  geom_segment(aes(x = 3, xend = 3, y = 0, yend = .005)) +
  annotate("text", x = -.5, y = .2, label = "34%") +
  annotate("text", x = .5, y = .2, label = "34%") +
  annotate("text", x = -1.5, y = .05, label = "14%") +
  annotate("text", x = 1.5, y = .05, label = "14%") +
  annotate("text", x = -2.5, y = .05, label = "2%") +
  annotate("text", x = 2.5, y = .05, label = "2%") +
  annotate("text", x = -3.5, y = .05, label = "<1%") +
  annotate("text", x = 3.5, y = .05, label = "<1%") +
  scale_x_continuous(limits = c(-4, 4),
                     breaks = c(seq(-4, 4, 1))) +
  ylim(0, 0.4) +
  theme_classic()

```

---

# Normal Distributions
### Highlighht area under the curve
We can use our understanding of the normal distribution to calculate the probability of observing a value in a certain range. This is known as the **empirical rule**.

```{r}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm) +
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = .38)) +
  geom_segment(aes(x = -1, xend = -1, y = 0, yend = .24)) +
  geom_segment(aes(x = 1, xend = 1, y = 0, yend = .24)) +
  annotate("text", x = -.5, y = .2, label = "34%") +
  annotate("text", x = .5, y = .2, label = "34%") +
  geom_label(x = 0, y = 0.1, label = "68%") +
  scale_x_continuous(limits = c(-4, 4),
                     breaks = c(seq(-4, 4, 1))) +
  ylim(0, 0.4) +
  theme_classic()

```

* A randomly selected observation has an approximately **68% chance of being within 1 SD** of the mean.
 + $P(-1 < Z < 1) = 0.683$
 
---
# Normal Distributions

```{r}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm) +
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = .38)) +
  geom_segment(aes(x = -2, xend = -2, y = 0, yend = .056)) +
  geom_segment(aes(x = 2, xend = 2, y = 0, yend = .056)) +
  annotate("text", x = -.5, y = .2, label = "34%") +
  annotate("text", x = .5, y = .2, label = "34%") +
  geom_label(x = 0, y = 0.1, label = "95.5%") +
  scale_x_continuous(limits = c(-4, 4),
                     breaks = c(seq(-4, 4, 1))) +
  ylim(0, 0.4) +
  theme_classic()
```

* A randomly selected observation has an approximately **95.5% chance of being within 2 SD** of the mean.
 + $P(-2 < Z < 2) = 0.955$

---
# What is normal?

* A normal distribution can take on any mean and standard deviation.

```{r}
ggplot(data.frame(x = c(-20, 20)), aes(x = x)) +
  stat_function(fun = dnorm, args = c(mean = 10, sd = 1), color = "red") +
    stat_function(fun = dnorm, args = c(mean = 3, sd = 3), color = "black") +
    stat_function(fun = dnorm, args = c(mean = -2, sd = 7), color = "blue") +
  ggtitle("Normal Distributions") +
  theme_classic()

```
* The area under the curve is always equal to 1.
* The empirical rule applies to all normal distributions (e.g., 68% of observations occur within 1 SD of the mean).
---

# Standard Normal Distribution

* The **standard** normal distribution has a mean of 0 and a standard deviation of 1.
  
```{r}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm) +
  theme_classic()

```

* The x-axis represents z-scores, meaning an observation's value represents how far it is from the mean.

  + A z-score of -1 indicates that an observation is 1 SD below the mean.
  + A z-score of +2 indicates that an observation is 2 SD above the mean.

* With a standard normal distribution, we can easily calculate the percentage of observations under any region.

---
# Z-Test
## ADD z-test code
Suppose we wanted to know the probability of scoring at least 1.5 SD above the mean on a standardized test. 

```{r}
ztest_plot <- ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm) +
  geom_vline(xintercept = 1.5, color = "red") +
  stat_function(fun = dnorm,
                xlim = c(1.5, 4),
                fill = "red",
                geom = "area") +
  theme_classic()

ztest_plot
```
To answer this question, we just need to find the area of the shaded portion under the curve.

---
# Z-Test

* We can either use a computational tool or a z-score look-up table to calculate the area under the normal curve.

```{r}
ztest_plot +
    annotate("text", x = 2, y = .02, label = ".07")

```

Either approach would show the area above 1.5 SD of a standard normal curve is .07. Therefore, the probability of scoring above 1.5 SD on a standardized test is 7%.

---
# Z-Test
Let's try it with our WHO data:
* Suppose that the population mean national life expectancy is 71.64 $(\mu)$ with a SD of 8.15 $(\sigma)$.

* **Assuming a normal distribution, what is the probability of randomly selecting a country with a life expectancy greater than 76?**

* First we need to standardize our value before we can calculate the percentage of observations above our z-score.

$$ z = \frac{76-71.64}{8.15} = 0.53$$

```{r}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm) +
  stat_function(fun = dnorm,
                xlim = c(0.53, 4),
                fill = "red",
                geom = "area") +
  geom_vline(xintercept = 0.53) +
  geom_label(aes(
    x = 0.53,
    y = +Inf,
    vjust = 3,
    label = paste("z = 0.53"))) +
  scale_x_continuous(limits = c(-4, 4),
                     breaks = c(seq(-4, 4, 1))) +
  ylim(0, 0.4) +
  theme_minimal()
```


---
# Z-Test

* The area under the curve above a z-score of 0.53 is .30. Therefore, the probability of randomly selecting a country with a national life expectancy greater than 76 is 30%.

```{r}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm) +
  stat_function(fun = dnorm,
                xlim = c(0.53, 4),
                fill = "red",
                geom = "area") +
  geom_vline(xintercept = 0.53) +
  geom_label(aes(
    x = 0.53,
    y = +Inf,
    vjust = 3,
    label = paste("z = 0.53"))) +
  annotate("text", x = 1.25, y = .05, label = "30%") +
  scale_x_continuous(limits = c(-4, 4),
                     breaks = c(seq(-4, 4, 1))) +
  ylim(0, 0.4) +
  theme_classic()
```

---
# Population vs. Sample Distributions
 
Using the WHO dataset, let's say we want to identify the mean life expectancy across all countries. However, let's also assume we only have the resources to sample from 10 countries.

.pull-left[

Our **population statistic** represents the true value across all countries.

```{r, echo = F}
world <- map_data("world")

regions <- world %>% 
  distinct(region) %>% 
  sample_n(10) %>% 
  mutate(sample = 1)

world <- left_join(world, regions) %>% 
  mutate(population = 1)
```

```{r, echo = F, fig.height=3, fig.width= 4}
ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region, fill = as.factor(population)),
    color = "black", size = 0.1
  ) +
  theme(legend.position = "none")
```
]

.pull-right[
Our **sample statistic** will be our estimate of the population statistic using the countries we examine.

```{r, echo = F, fig.height = 3, fig.width = 4}
ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region, fill = as.factor(sample)),
    color = "black", size = 0.1
  ) +
  theme(legend.position = "none")
```
]

---
# Sample Estimates

* Assume that our <span style="color: blue;">true population mean </span> is 71.64. 

* Each possible random sample of 10 countries will produce its own <span style="color: red;"> sample mean or population mean estimate </span>.

* These estimated means have their own variability around the true population mean.

```{r, echo = F}
sim_num <- seq(1:1000)
region_samp <- vector("list", 1000)
for (i in seq_along(region_samp)){
    region_samp[[i]] <- who %>% 
      filter(str_length(region) <= 12) %>% 
      sample_n(10)
}

names(region_samp) <- paste0("sim_", sim_num)
```

```{r, echo = F, fig.width = 8, fig.height = 5}
plots <- lapply(region_samp[1:4], function(x) {
  ggplot(x, aes(x = region, y = life_expectancy)) +
    geom_point() +
    geom_hline(
      yintercept = mean(x$life_expectancy),
      color = "red",
      linetype = "dotted"
    ) +
    geom_hline(yintercept = mean(who$life_expectancy),
               color = "blue") +
    geom_label(aes(
      x = +Inf,
      y = mean(x$life_expectancy),
      vjust = 2,
      label =  paste(mean(x$life_expectancy))
    ),
    colour = "red",
    size = 4) +
    ylim(50, 85) +
    coord_flip() +
    theme_minimal()
})
library(gridExtra)
marrangeGrob(plots, nrow=2, ncol=2, top = paste("Estimated Population Means Across Four Different Samples"))
```

---
# Population vs. Sample Distribution

If we sample our population **100 times** and plot the estimated mean of each sample, the estimated means begin to form their own normal distribution. The mean of sample means is our <span style="color: orange;"> **estimated population mean**. </span>.

```{r, echo = F}
samp_means <- tibble(samp = 1:1000, means = map_dbl(region_samp, ~mean(.$life_expectancy)))
```


```{r, animation.hook="gifski", interval = 0.4, echo = F, eval = T}
for (i in 1:10) {
  x <- ggplot(subset(samp_means, samp <= i), aes(means)) +
    geom_histogram(fill = "darkred", color = "grey") +
    geom_vline(xintercept = 71.64, color = "blue") +
    geom_vline(xintercept = mean(samp_means$means[samp_means$samp <= i]), color = "orange") +
    geom_label(aes(
      x = 71.64,
      y = +Inf,
      vjust = 2,
      label = 71.64
    ),
    colour = "blue",
    size = 4) +
        geom_label(aes(
      x = mean(samp_means$means[samp_means$samp <= i]),
      y = +Inf,
      vjust = 3,
      label = paste(round(mean(samp_means$means[samp_means$samp <= i]), 2))
    ),
    colour = "orange",
    size = 5) + 
    xlim(65, 85) +
    xlab("Mean Life Expectancy") +
    theme_minimal()
  print(x)
}

```

The mean of our sampling distribution (mean of means) approximates the <span style="color: blue;"> population mean</span>. This is an example of the **central limit theorem**.

If we sampled all possible samples of size *n* from our population, our mean of sample means would be equal to the population mean.

---
# Central Limit Theorem

> Given a population mean $\mu$ and standard deviation of $\sigma$, the sampling distribution of the mean is a normal distribution with a mean equal to $\mu$ and standard deviation equal to $\frac{\sigma}{\sqrt{n}}$, where *n* is the sample size.

> The distribution of sample means will approach the normal distribution as the sample size increases regardless of the shape of the population distribution.

* The sampling distribution of means approximates a normal distribution because it is estimating a statistic of the population distribution. It is not "recreating" the shape of the population distribution.

* Center: Mean of sample means is the population mean.
$$ \mu_\bar{X} = \mu$$

* Spread: Standard deviation of sample means is the standard error of the mean

$$ \sigma_\bar{X} = \frac{\sigma}{\sqrt{n}}$$

```{r, eval = F, echo = F}
for (i in seq(1, 100, 10)) {
 x <- ggplot(who, aes(life_expectancy)) +
     geom_histogram(fill = "darkred", color = "grey", bins = 30) +
     geom_vline(xintercept = 71.64, color = "blue") +
     geom_vline(xintercept = mean(samp_means$means[samp_means$samp <= i]), color = "green") +
     geom_label(aes(
       x = 71.64,
       y = +Inf,
       vjust = 2,
       label = 71.64
     ),
     colour = "blue",
     size = 4) +
     xlim(50, 85) +
   xlab("Mean Life Expectancy") +
     theme_minimal()
   print(x)
 }

```

---
# Population vs. Sample Notation


---
# Central Limit Theorem
## MOVE EARLIER

* Here is a histogram of the means of 1000 random samples from our WHO data.
* The center of the distribution is our population mean.
* The standard deviation is the standard error of the mean.
{https://onlinestatbook.com/stat_sim/sampling_dist/ }

```{r, parse = T, echo = F}

ggplot(samp_means, aes(means)) +
  geom_histogram() +
  geom_vline(xintercept = mean(samp_means$means), color = "red") +
  geom_segment(
    x = mean(samp_means$means),
    xend = (mean(samp_means$means) + sd(samp_means$means)),
    y = 30,
    yend = 30,
    color = "red"
  ) +
  geom_label(aes(
    x = mean(samp_means$means),
    y = +Inf,
    vjust = 3,
    label = paste('mu', "==", round(mean(samp_means$means), 2))
  ), parse = T) +
    geom_label(aes(
    x = (mean(samp_means$means) + sd(samp_means$means)),
    y = +Inf,
    vjust = 6,
    label = paste('sigma', "==", round(sd(samp_means$means), 2))
  ), parse = T) +
  theme_classic()
  

```

---
SUMMARY SLIDE ON WHAT THE CENTRAL LIMIT THEOREM ALLOWS US TO DO
---
# T-Tests
* One-Sample T-Test
* T-Distributions
* Applied Example
* Hypothesis Testing

---
# Population Variance
* In a z-test, the population standard deviation is assumed to be known.

* In practice, we **rarely** know the true population standard deviation due to:
  + Measurement Error
  + Population statistics can be in constant flux (e.g., average life expectancies can change moment to moment)
  + Can be unfeasible to measure entire population

---
# T-Distributions
* Assume that the population standard deviation $(\sigma)$ is unknown. 
* Resemble Z-distributions as degrees of freedom increase.
* The degrees of freedom for a t-test is always *n*-1, where *n* is your sample size.

* comes from the central limit theorem
DESCRIBE HOW IT EMERGES FROM CENTRAL LIMIT THEOREM
  
```{r, echo=F, animation.hook="gifski", interval=0.4,}

df_seq <- tibble(df = seq(1, 30, 3))

for (i in 1:nrow(df_seq)) {
x <- ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm,
                aes(color = "normal")) +
  stat_function(fun = dt, 
                args = list(df = df_seq$df[i]),
                color = "green") +
  ggtitle("T-Distributions",
          subtitle = paste("df =", df_seq$df[i])) +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))
print(x)
}
```



```{r , echo=F, animation.hook="gifski", interval=0.4, eval = F}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm,
                aes(color = "normal")) +
  stat_function(fun = dt, 
                args = list(df = 5), 
                aes(color = "5")) +
  stat_function(fun = dt,
                args = list(df = 10),
                aes(color = "10")) +
  stat_function(fun = dt,
                args = list(df = 30),
                aes(color = "30")) +
  scale_colour_brewer("Degrees of Freedom", 
                      palette = "Set1",
                      limits = c("Normal", "5", "10", "30")) +
  ggtitle("T-Distributions") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))


```

---
# One Sample T-Test
* T-tests are one of the most foundational statistical tests in classical statistics.

* The purpose of a one-sample t-test is to describe the probability of obtaining a particular sample mean through random sampling variability, assuming the population mean = K.
  + e.g. Is our observed sample mean to be expected, assuming it reflects a random sample of the population?
  
  
FOR EXAMPLE, SUPPOSE WE WANT TO KNOW IF A SAMPLE OF COUNTRIES IS DIFFERENT FROM THE KNOWN POPULATION'S AVERAGE LIFE EXPECTANCY.

---
* Does **not** tests probability of obtaining the sample distribution or data itself, only the sample mean.
  
* **Null Hypothesis** $(H_0)$: $\mu = K$, where $K$ is any constant.

---
# One-Sample T-Test

* The t-statistic is calculated like a z-score, but we replace the population standard deviation with the sample standard deviation.

$$ t = \frac{\bar{X} - \mu}{\hat{\sigma}/\sqrt{n}} $$

.pull-left[
* Distributional spread and proportion of the area under the curve beyond our t-value depends on our degrees of freedom.

* Using a t-test, we can calculate the probability of obtaining a particular sample mean through random sampling.
]

.pull-right[
```{r , echo=F, animation.hook="gifski", interval=0.4}
samp_means <- tibble(df = seq(2, 20, 2), 
                     val = round(
                       map_dbl(seq(2, 20, 2), 
                                   ~ 2*pt(-abs(1), df = .x)),
                       digits = 3))

for (i in 1:10) {
  x <- ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = samp_means$df[i]), 
                color = "black") +
  stat_function(fun = dt, 
                args = list(df = samp_means$df[i]), 
                xlim = c(1,4),
                fill = "red",
                geom = "area") +
          geom_label(aes(
      x = 1,
      y = 0.3,
      label = "t = 1.0")) +
      geom_label(aes(
      x = 1.5,
      y = 0.04,
      label = samp_means$val[i])) +
  geom_vline(xintercept = 1) +
  ggtitle("T-Distribution", 
                subtitle = paste("Degrees of Freedom =", samp_means$df[i])) +
    xlab("T-Statistic")
  print(x)
}
```
]
---
## Applying a t-test
Suppose we have data from a sample of 10 countries with a mean life expectancy of 66.87 and standard deviation of 5.82.

If our population mean is thought to be 78.16 $(H_0: \sigma = 78.16)$, what is the probability of obtaining a sample mean of 66.87, **or a more extreme mean value**, due to random sampling variability?
$$t(9) = \frac{66.87 - 75.16}{5.82/\sqrt{9}} = -1.24$$
```{r}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = 9), 
                color = "black") +
  geom_vline(xintercept = -1.24, color = "red") +
  geom_label(aes(
      x = -1.24,
      y = 0.2,
      label = paste("t = -1.24"))) +
    xlab("T-Statistic") +
  theme_minimal()

```

---
# Applying a t-test

Assuming a population mean of 78.16 and sample size of 10...

.pull-left[
```{r, echo = F}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = 9), 
                color = "black") +
  stat_function(fun = dt, 
                args = list(df = 9), 
                xlim = c(-4, -1.24),
                fill = "red",
                geom = "area") +
  geom_vline(xintercept = -1.24) +
  geom_label(aes(
      x = -1.5,
      y = 0.04,
      label = paste(round(pt(q= -1.24, df = 9, lower.tail = T), 2)))) +
    xlab("T-Statistic") +
  theme_minimal() +
  ggtitle("One-tailed t-test",
          subtitle = paste("p = ", round(pt(q= -1.24, df = 9, lower.tail = T), 2))) +
    theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

```

...the proportion of random samples that would demonstrate a sample mean **equal to or lower than 66.87** is 0.12 (i.e., *p* = 0.12).
]

.pull-right[
```{r, echo = F}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = 9), 
                color = "black") +
  stat_function(fun = dt, 
                args = list(df = 9), 
                xlim = c(-4, -1.24),
                fill = "red",
                geom = "area") +
    stat_function(fun = dt, 
                args = list(df = 9), 
                xlim = c(1.24, 4),
                fill = "red",
                geom = "area") +
  geom_vline(xintercept = -1.24) +
   geom_label(aes(
      x = -1.5,
      y = 0.04,
      label = paste(round(pt(q= -1.24, df = 9, lower.tail = T), 2)))) +
   geom_label(aes(
      x = 1.5,
      y = 0.04,
      label = paste(round(pt(q= 1.24, df = 9, lower.tail = F), 2)))) +
    xlab("T-Statistic") +
  theme_minimal() +
  ggtitle("Two-tailed t-test",
          subtitle = paste("p =", 2*round(pt(q= -1.24, df = 9, lower.tail = T), 2))) +
    theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))
```
...the proportion of random samples that would demonstrate a sample mean **equal to or more extreme than 66.87** is 0.24 (i.e., *p* = 0.24).
]

**Note:** One-tailed tests are rarely warranted and we will always be conducting two-tailed tests.

> If the null hypothesis is true, the probability of obtaining a sample mean equal to or more extreme 66.87 is 0.24. What does this mean for the null hypothesis?

---
ONE-TAILED VS. TWO-TAILED TESTS
we are testing whether an extreme value would occur on either side of the mean value. If it is equally likely 
---
# Alpha Thresholds and t-values

## NEEDS ELABORATION
* Critical t-values $t_{\alpha/2}(df)$ are the thresholds at which our alpha threshold is met.
  + Vary depending on $df$ and $\alpha$
  
```{r, echo = F}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = 9), 
                color = "black") +
  stat_function(fun = dt, 
                args = list(df = 9), 
                xlim = c(-4, -2.2622),
                fill = "red",
                geom = "area") +
    stat_function(fun = dt, 
                args = list(df = 9), 
                xlim = c(2.2622, 4),
                fill = "red",
                geom = "area") +
  geom_vline(xintercept = -2.2622) +
  geom_vline(xintercept = 2.2622) +
     geom_label(aes(
      x = -2.2622,
      y = 0.2,
      label = paste("Critical T = -2.2622"))) +
   geom_label(aes(
      x = 2.5,
      y = 0.2,
      label = paste("Critical T = 2.2622"))) +
    xlab("T-Statistic") +
   geom_label(aes(
      x = -2.5,
      y = 0.04,
      label = paste(round(pt(q= -2.2622, df = 9, lower.tail = T), 4)))) +
   geom_label(aes(
      x = 2.5,
      y = 0.04,
      label = paste(round(pt(q= 2.2622, df = 9, lower.tail = F), 4)))) +
    xlab("T-Statistic") +
  theme_minimal() +
  ggtitle("Two-tailed t-test",
          subtitle = paste("p =", 2*round(pt(q= -2.2622, df = 9, lower.tail = T), 4)))
```

With two-tailed tests, the total alpha is "split" across both tails of the distribution $(\alpha(.05)/2 = .025)$.

---

* Assuming we had set our $\alpha$ at .05, let's see if we reject our null hypothesis in our previous example.

.pull-left[
Our observed t-value did not surpass our critical t-values.
```{r}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = 9), 
                color = "black") +
  geom_vline(xintercept = -1.24, color = "red") +
  geom_label(aes(x = -1.24, y = 0.3, 
                 label = "Observed T")) +
  geom_vline(xintercept = -2.2622) +
  geom_vline(xintercept = 2.2622) +
     geom_label(aes(
      x = -2.2622,
      y = 0.2,
      label = paste("Critical T"))) +
   geom_label(aes(
      x = 2.5,
      y = 0.2,
      label = paste("Critical T"))) +
    xlab("T-Statistic") +
  theme_minimal() +
  ggtitle("Two-tailed t-test",
          subtitle = paste("p =", 2*round(pt(q= -1.24, df = 9, lower.tail = T), 2))) +
    theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

```
]

.pull-right[
Our p-value is not less than .05 and does not satisfy our alpha threshold.
```{r}

ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, 
                args = list(df = 9), 
                color = "black") +
  stat_function(fun = dt, 
                args = list(df = 9), 
                xlim = c(-4, -1.24),
                fill = "red",
                geom = "area") +
    stat_function(fun = dt, 
                args = list(df = 9), 
                xlim = c(1.24, 4),
                fill = "red",
                geom = "area") +
  geom_vline(xintercept = -1.24) +
   geom_label(aes(
      x = -1.5,
      y = 0.04,
      label = paste(round(pt(q= -1.24, df = 9, lower.tail = T), 2)))) +
   geom_label(aes(
      x = 1.5,
      y = 0.04,
      label = paste(round(pt(q= 1.24, df = 9, lower.tail = F), 2)))) +
    xlab("T-Statistic") +
  theme_minimal() +
  ggtitle("Two-tailed t-test",
          subtitle = paste("p =", 2*round(pt(q= -1.24, df = 9, lower.tail = T), 2))) +
    theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))
```
]

Assuming a population mean of 78.16 and sample size of 10, the proportion of random samples that would demonstrate a sample mean **equal to or more extreme than 66.87** is 0.24 $(t = -1.24, p = 0.24)$. Because our p-value does not meet our alpha threshold of .05, we **fail to reject the null hypothesis**.

<<<<<<< HEAD
Thus, we can conclude that the mean life expectancy of this sample of countries is not different from the mean population average.

---
# Independent Two-Sample t-test

* An independent samples t-test is a comparison of the means from two different groups.

* Enables a researchers to test for a significant difference between two groups.
 + e.g., Does the life expectancy of high-income countries significantly differ from low-income countries?


## Elaborate on motivation for test


---
* Independent samples means a subject can only belong to one group. 

$$ H_0: \mu_1 = \mu_2 $$
$$ H_1: \mu_1 \neq \mu_2 $$
---
# Independent Two-Sample t-test

If our null hypothesis is $H_0: \mu_1 = \mu_2$, then $\mu_1 - \mu_2$ should also equal 0.

To calculate our test statistic we use the following formula:

$$ t = \frac{\bar{X}_1 - \bar{X}_2}{SE} $$

Our standard error (SE) refers to the $\sigma$ for the sampling distribution of the **difference in means**. This is a multi-step process to calculate.

Assuming there is no difference between our sample means (null hypothesis is true), our t will always equal zero. 

$$ t = \frac{0}{SE} = 0 $$

---
# Sampling a difference in means

Population X - $\mu_X$

Population Y- $\mu_Y$

$$\mu_X - \mu_Y$$
Sample 1 of Group X - $\bar{x}_1$

Sample 1 of Group Y - $\bar{y}_1$

$$\bar{x}_1 - \bar{y}_1$$
Sample 2 of Group X - $\bar{x}_2$

Sample 2 of Group Y - $\bar{y}_2$

$$\bar{x}_2 - \bar{y}_2$$
---
# Sampling a difference in means

ADD LEGEND OR OTHER WAY TO LABEL DISTRIUBTIONS BY THEIR GROUP

Suppose we have two populations of students - students with infrequent school attendance and students who attend school regularly. On our standardized test, students with infrequent attendance have an average score of 95 $\mu_x = 95)$ and students with regular attendance have an average score of 104 $\mu_x = 104)$. Thus, our true difference in means is 9.
```{r}
sim_2pop <- tibble(x = rnorm(1000, mean = 95, sd = 10),
                    y = rnorm(1000, mean = 104, sd = 10))


sim_2samp <- map(1:30, ~sim_2pop %>% 
                   sample_n(40))

sim_means <- tibble(samp = seq(1, 30, 1),
                    x = map_dbl(sim_2samp, ~mean(.$x)),
                    y = map_dbl(sim_2samp, ~mean(.$y)),
                    diff = x - y)
```

Just like with our one-sample t-test, every time we sample each population, we obtain a new estimated difference between the two groups.

```{r , echo=F, animation.hook="gifski"}

for (i in seq(1:30)) {

dist_plots <- ggplot(sim_2samp[[i]]) +
  geom_density(aes(x = sim_2samp[[i]]$x), color = "red") +
  geom_density(aes(x = sim_2samp[[i]]$y), color = "blue") +
  geom_vline(xintercept = mean(sim_2samp[[i]]$x), color = "red") +
  geom_vline(xintercept = mean(sim_2samp[[i]]$y), color = "blue") +
  geom_segment(x = mean(sim_2samp[[i]]$x), xend = mean(sim_2samp[[i]]$y), y = 0.015, yend = 0.015, linetype = 2) +
  geom_label(x = ((mean(sim_2samp[[i]]$x) + mean(sim_2samp[[i]]$y))/2), 
             y = 0.01, 
             label = paste(round(sim_means$diff[i]), 2)) +
  theme_minimal()
 print(dist_plots)
 
}
```

---

As we take repeated samples, the estimated difference in means forms its own normal distribution.

.pull-left[
```{r , echo=F, animation.hook="gifski"}

for (i in seq(1:30)) {

dist_plots <- ggplot(sim_2samp[[i]]) +
  geom_density(aes(x = sim_2samp[[i]]$x), color = "red") +
  geom_density(aes(x = sim_2samp[[i]]$y), color = "blue") +
  geom_vline(xintercept = mean(sim_2samp[[i]]$x), color = "red") +
  geom_vline(xintercept = mean(sim_2samp[[i]]$y), color = "blue") +
  geom_segment(x = mean(sim_2samp[[i]]$x), xend = mean(sim_2samp[[i]]$y), y = 0.015, yend = 0.015, linetype = 2) +
  geom_label(x = ((mean(sim_2samp[[i]]$x) + mean(sim_2samp[[i]]$y))/2), 
             y = 0.01, 
             label = paste(round(sim_means$diff[i]), 2)) +
  theme_minimal()
 print(dist_plots)
 
}
```
]

.pull-right[
```{r , echo=F, animation.hook="gifski"}
for (i in seq(1:30)) {
mean_plots <- ggplot(subset(sim_means, samp <= i), aes(diff)) +
    geom_histogram(fill = "black", color = "grey") +
    geom_vline(xintercept = -9, color = "blue") +
    geom_vline(xintercept = mean(sim_means$diff[sim_means$samp <= i]), color = "orange") +
    geom_label(aes(
      x = -9,
      y = +Inf,
      vjust = 2,
      label = -9
    ),
    colour = "blue",
    size = 4) +
        geom_label(aes(
      x = mean(sim_means$diff[sim_means$samp <= i]),
      y = +Inf,
      vjust = 3,
      label = paste(round(mean(sim_means$diff[sim_means$samp <= i]), 2))
    ),
    colour = "orange",
    size = 4) + 
    xlab("Mean Difference") +
    theme_minimal()

print(mean_plots)
 }

```
]
---
ADD SLIDE THAT SHOWS CONVERGENCE TO NORMAL DISTRIUBTION


---
# Central Limit Theorem

Given two populations such that the mean is $\mu_X$ and standard deviation is $\sigma$ for Population 1 and the mean is $\mu_Y$ and standard deviation is $\sigma$ for Population 2, the sampling distribution of the difference in sample means is a normal distribution with a mean equal to $\mu_X - \mu_Y$ and standard deviation equal to $\sqrt{\frac{\sigma^2}{n_1} + \frac{\sigma^2}{n_2}}$, where $n_1$ and $n_2$ are the sample sizes.

Notice we have only one $\sigma$ for both groups. This is an assumption of **homogeneity of variance**, meaning they share the same population variance despite differences in their means.

If we do not have homogeneity of variance, then SOMETHING ABOUT WELCH'S T.
---
# Estimating Variance

CHANGE S TO SIGMA HAT SQUARED
Since we rarely know the population standard deviation, we have to calculate a **pooled standard deviation** to estimate the populations standard deviation.

$$\hat{\sigma} = \sqrt{\frac{(n_1-1)s_1 + (n_2-1)s_2 }{n_1+n_2-2}} $$
* Sample sizes do not need to be equal.

* Our degrees of freedom for an independent sample t-test is represented by $n_1+n_2-2$.

UTILIZE OLD VARIANCE SLIDES

WE WANT TO ENSURE THAT EACH UNIT IS EQUALLY WEIGHTED (THE SD OF ONE GROUP IS NOT OVER-REPRESENTED IF IT HAS A SMALLER N SIZE)

---
# Calculating the Standard Error

Our standard error of the difference in means $(\sigma_{diff})$ is calculated from our pooled sample standard deviation $(\hat{\sigma}_)$ and sample sizes.

$$SE(\sigma_{diff}) = \sqrt{\frac{\hat{\sigma}^2}{n_1} + \frac{\hat{\sigma}^2}{n_2}} $$

Then we just have to insert our standard error into our formula to find our test statistic.

$$ t = \frac{\bar{X}_1 - \bar{X}_2}{SE} = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{\hat{\sigma}^2}{n_1} + \frac{\hat{\sigma}^2}{n_2}}}$$
---
# Independent Samples T-Test

## cycle through slides with color coding to build final equation
Null Hypothesis: Students with infrequent attendance have equal achievement scores to students with regular attendance.
$$ H_0: \mu_1 = \mu_2 $$
* Sample Sizes:
  + $n_1 = 12$
  + $n_2 = 76$
* Means:
  + $\bar{X_1} = 94.78$
  + $\bar{X_2} = 103.22$
* Sample Variance:
  + $s_1 = 11.59$
  + $s_2 = 10.26$
  
$$\hat{\sigma} = \sqrt{\frac{(12-1)11.59 + (76-1)103.22 }{12+76-2}} = 9.57 $$
Observed Mean Difference $(\bar{X}_1 - \bar{X}_2)$ = $94.78 - 103.22 = -8.44$

---
# Independent Samples T-Test
**Null Hypothesis:** Students with infrequent attendance have equal achievement scores to students with regular attendance.

$$ H_0: \mu_1 = \mu_2 $$

Pooled variance $(\hat{\sigma})$ = 9.57


Sample Mean Difference $(\bar{X}_1 - \bar{X}_2)$ = $-8.44$

$$  t = \frac{\bar{X}_1 - \bar{X}_2}{SE} = \frac{-8.44}{\sqrt{\frac{{9.57}^2}{12} + \frac{{9.57}^2}{76}}} = \frac{-8.44}{2.97} - -2.84$$
> If the null hypothesis is true, what is the probability of observing a t-statistic of -2.84, or a more extreme value (degrees of freedom = 86)?

---
# Evaluating the Null Hypothesis

The p-value for a two-tailed test with a t-value of -2.84 and degrees of freedom is .006. 
```{r}
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt,
                args = list(df = 86)) +
  geom_vline(xintercept = -2.84, color = "red") +
  stat_function(
    fun = dt,
    args = list(df = 86),
    xlim = c(-4,-2.84),
    fill = "red",
    geom = "area"
  ) +
    stat_function(
    fun = dt,
    args = list(df = 86),
    xlim = c(2.84, 4),
    fill = "red",
    geom = "area"
  ) +
  ggtitle("Distribution of the Difference in Means",
  "t-value = -2.84, df = 86, p = .006") +
  theme_minimal()
  
```

Assuming the null hypothesis is true, the probability of obtaining a difference in sample means of +/- 8.44 or greater is .006. Because this is highly unlikely (p = .006 < .05), we can **reject the null hypothesis**.

ADD SUMMARY STATEMENT HERE
---
# Confidence Intervals
